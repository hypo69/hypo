# Модуль `provider.py`

## Обзор

Модуль `provider.py` предназначен для работы с локальными моделями GPT4All. Он предоставляет функциональность для загрузки, поиска и использования этих моделей для генерации текста. Модуль включает в себя функции для поиска каталога с моделями, создания завершений (completions) на основе локальных моделей и обработки запросов.

## Подробнее

Этот модуль является частью проекта `hypotez` и обеспечивает возможность использования локальных моделей GPT4All для генерации текста. Он выполняет поиск моделей в различных директориях, загружает их при необходимости и предоставляет интерфейс для создания завершений на основе этих моделей. Это позволяет использовать модели GPT4All без необходимости подключения к внешним сервисам.

## Функции

### `find_model_dir`

```python
def find_model_dir(model_file: str) -> str:
    """
    Определяет путь к каталогу, содержащему файл модели.

    Args:
        model_file (str): Имя файла модели.

    Returns:
        str: Путь к каталогу, содержащему файл модели.
    """
```

**Назначение**: Функция `find_model_dir` определяет путь к каталогу, в котором находится указанный файл модели. Она ищет модель в нескольких местах: в локальной директории, в директории проекта и в текущей рабочей директории.

**Как работает функция**:

1. **Определение путей**: Определяются пути к возможным расположениям файла модели:
   - `local_dir`: Директория, где находится текущий файл (`provider.py`).
   - `project_dir`: Родительская директория `local_dir`.
   - `new_model_dir`: Предполагаемая директория для моделей в структуре проекта.
   - `new_model_file`: Полный путь к файлу модели в `new_model_dir`.
   - `old_model_dir`: Старая директория для моделей, расположенная в `local_dir`.
   - `old_model_file`: Полный путь к файлу модели в `old_model_dir`.
   - `working_dir`: Текущая рабочая директория.

2. **Поиск файла модели**:
   - Проверяется наличие файла модели в `new_model_file`. Если файл существует, функция возвращает `new_model_dir`.
   - Если файл не найден в `new_model_file`, проверяется его наличие в `old_model_file`. Если файл существует, функция возвращает `old_model_dir`.
   - Если файл не найден ни в `new_model_file`, ни в `old_model_file`, выполняется поиск файла в текущей рабочей директории и её поддиректориях с использованием `os.walk`. Если файл найден, функция возвращает корневую директорию, в которой он был найден.

3. **Возврат значения**:
   - Если файл модели не найден ни в одном из указанных мест, функция возвращает `new_model_dir` как директорию по умолчанию.

**ASCII Flowchart**:

```
Определение путей (local_dir, project_dir, new_model_dir, new_model_file, old_model_dir, old_model_file, working_dir)
│
└──> Проверка наличия файла в new_model_file
│    │
│    └──> Если файл найден: возврат new_model_dir
│    │
└──> Проверка наличия файла в old_model_file
│    │
│    └──> Если файл найден: возврат old_model_dir
│    │
└──> Поиск файла в текущей рабочей директории (working_dir)
│    │
│    └──> Если файл найден: возврат корневой директории
│    │
└──> Возврат new_model_dir (как директории по умолчанию)
```

**Примеры**:

```python
model_file = "ggml-model.bin"
model_dir = find_model_dir(model_file)
print(model_dir)
```

### `LocalProvider.create_completion`

```python
    @staticmethod
    def create_completion(model: str, messages: Messages, stream: bool = False, **kwargs):
        """
        Создает завершение (completion) на основе локальной модели GPT4All.

        Args:
            model (str): Имя модели.
            messages (Messages): Список сообщений для передачи модели.
            stream (bool): Флаг, указывающий, следует ли использовать потоковую передачу. По умолчанию `False`.
            **kwargs: Дополнительные аргументы.

        Returns:
            Generator[str, None, None] | str: Генератор токенов (если `stream=True`) или строку завершения (если `stream=False`).

        Raises:
            ValueError: Если модель не найдена или не реализована.
        """
```

**Назначение**: Метод `create_completion` создает завершение текста на основе локальной модели GPT4All. Он принимает имя модели, список сообщений и флаг потоковой передачи, а затем возвращает результат в виде генератора токенов или строки завершения.

**Как работает функция**:

1. **Инициализация**:
   - Проверяется, инициализирован ли глобальный словарь моделей `MODEL_LIST`. Если нет, он инициализируется вызовом функции `get_models()`.
   - Проверяется, существует ли запрошенная модель в `MODEL_LIST`. Если модель не найдена, вызывается исключение `ValueError`.

2. **Подготовка к загрузке модели**:
   - Извлекается информация о модели из `MODEL_LIST`.
   - Определяется путь к файлу модели с помощью функции `find_model_dir`.
   - Проверяется наличие файла модели по определенному пути. Если файл не найден, пользователю предлагается скачать модель. В случае отказа вызывается исключение `ValueError`.

3. **Загрузка и настройка модели**:
   - Создается экземпляр класса `GPT4All` с указанием имени файла модели, пути к модели и других параметров.

4. **Формирование контекста для модели**:
   - Извлекаются системные сообщения из списка сообщений и формируется строка `system_message`. Если системных сообщений нет, используется сообщение по умолчанию.
   - Формируется строка `conversation` из пользовательских и ассистентских сообщений.

5. **Генерация ответа**:
   - Определяется функция `should_not_stop`, которая определяет, следует ли остановить генерацию на основе текущего токена.
   - Запускается сессия чата с моделью, используя `system_message` и `prompt_template`.
   - В зависимости от значения флага `stream`, генерируется либо потоковый, либо не потоковый ответ.
   - Если `stream=True`, функция возвращает генератор токенов. В противном случае функция возвращает строку завершения.

**ASCII Flowchart**:

```
Проверка инициализации MODEL_LIST
│
└──> Если MODEL_LIST не инициализирован: инициализация через get_models()
│
└──> Проверка наличия модели в MODEL_LIST
│    │
│    └──> Если модель не найдена: вызов ValueError
│    │
└──> Определение пути к файлу модели через find_model_dir
│    │
│    └──> Проверка наличия файла модели
│         │
│         └──> Если файл не найден: предложение скачать модель
│              │
│              └──> Если пользователь отказывается: вызов ValueError
│
└──> Создание экземпляра GPT4All
│
└──> Формирование system_message и conversation
│
└──> Запуск сессии чата и генерация ответа
│    │
│    └──> Если stream=True: генерация потокового ответа
│    │
│    └──> Если stream=False: генерация не потокового ответа
│
└──> Возврат результата (генератор токенов или строка завершения)
```

**Примеры**:

```python
model_name = "ggml-model.bin"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is the capital of France?"}
]
stream = False
completion = LocalProvider.create_completion(model_name, messages, stream)
print(completion)
```
```python
model_name = "ggml-model.bin"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is the capital of France?"}
]
stream = True
for token in LocalProvider.create_completion(model_name, messages, stream):
    print(token, end="", flush=True)