# Модуль files

## Обзор

Модуль предоставляет инструменты для работы с файлами различных форматов, такими как PDF, DOCX, ODT, EPUB, XLSX, HTML, ZIP и другие текстовые файлы. Он включает функции для безопасной обработки имен файлов, чтения содержимого файлов, кэширования, разделения на части, загрузки файлов из URL и извлечения ссылок из HTML.

## Подробней

Модуль предназначен для обработки и извлечения текста из различных типов файлов, которые могут быть загружены или доступны по URL. Он использует различные библиотеки для работы с каждым типом файлов, такие как PyPDF2, pdfplumber, pdfminer, docx, docx2txt, odfpy, ebooklib, pandas и BeautifulSoup4. Модуль также предоставляет функции для кэширования и разделения файлов на части для оптимизации обработки больших файлов.

## Функции

### `secure_filename`

```python
def secure_filename(filename: str) -> str:
    """
    Очищает имя файла, удаляя небезопасные символы.

    Args:
        filename (str): Имя файла для очистки.

    Returns:
        str: Очищенное имя файла.
    """
```

**Назначение**: Обеспечивает безопасность имени файла путем удаления или замены небезопасных символов.

**Параметры**:
- `filename` (str): Имя файла, которое необходимо очистить.

**Возвращает**:
- `str`: Очищенное имя файла, в котором все небезопасные символы заменены на безопасные.

**Как работает функция**:
1. Функция принимает имя файла в качестве входных данных.
2. Использует регулярное выражение для удаления всех символов, кроме букв, цифр, `.,_-+`.
3. Удаляет пробелы в начале и конце строки, заменяет несколько пробелов одним пробелом.
4. Обрезает строку до 100 символов и удаляет символы `.,_-+` в начале и конце строки.

**Примеры**:
```python
secure_filename("безпечне ім'я.txt") # 'txt'
secure_filename("文件名.txt") # 'txt'
secure_filename("test!@#$%^&*().txt") # 'test.txt'
```

### `supports_filename`

```python
def supports_filename(filename: str):
    """
    Проверяет, поддерживается ли указанный тип файла для обработки.

    Args:
        filename (str): Имя файла для проверки.

    Returns:
        bool: True, если файл поддерживается, иначе False.

    Raises:
        MissingRequirementsError: Если отсутствуют необходимые библиотеки для обработки файла.
    """
```

**Назначение**: Проверяет, может ли модуль обрабатывать файлы определенного типа на основе доступных библиотек.

**Параметры**:
- `filename` (str): Имя файла, тип которого необходимо проверить.

**Возвращает**:
- `bool`: `True`, если файл поддерживается для обработки, и `False` в противном случае.

**Вызывает исключения**:
- `MissingRequirementsError`: Если отсутствуют необходимые библиотеки для обработки определенного типа файла.

**Как работает функция**:
1. Проверяет расширение файла.
2. В зависимости от расширения файла, проверяет наличие необходимых библиотек (например, `PyPDF2` для `.pdf`, `docx` для `.docx`).
3. Если необходимые библиотеки отсутствуют, вызывает исключение `MissingRequirementsError`.
4. Если файл относится к plain text, проверяет, что имя файла не `FILE_LIST`, и возвращает `True`.

**Примеры**:
```python
supports_filename("example.pdf") # True, если установлен PyPDF2
supports_filename("document.docx") # True, если установлен python-docx
supports_filename("file.txt") # True
```

### `get_bucket_dir`

```python
def get_bucket_dir(*parts):
    """
    Формирует путь к каталогу bucket на основе переданных частей.

    Args:
        *parts: Переменное количество частей пути для формирования каталога bucket.

    Returns:
        str: Полный путь к каталогу bucket.
    """
```

**Назначение**: Создает путь к каталогу, используемому для хранения файлов bucket.

**Параметры**:
- `*parts`: Переменное количество аргументов, представляющих части пути к каталогу bucket.

**Возвращает**:
- `str`: Полный путь к каталогу bucket, сформированный из переданных частей.

**Как работает функция**:
1. Объединяет части пути, используя функцию `os.path.join`.
2. Для каждой части пути применяет функцию `secure_filename` для обеспечения безопасности имени файла.

**Примеры**:
```python
get_bucket_dir("my_bucket", "file.txt") # os.path.join(get_cookies_dir(), "buckets", "my_bucket", "file.txt")
```

### `get_buckets`

```python
def get_buckets():
    """
    Получает список директорий buckets.

    Returns:
        list[str] | None: Список имен директорий buckets или None, если директория не существует.
    """
```

**Назначение**: Получает список всех доступных каталогов bucket.

**Возвращает**:
- `list[str] | None`: Список имен каталогов bucket, если они существуют. Возвращает `None`, если базовая директория не существует.

**Как работает функция**:
1. Получает путь к основной директории buckets.
2. Пытается получить список всех директорий в этой директории.
3. Возвращает список директорий или `None`, если основная директория не существует.

**Примеры**:
```python
get_buckets() # ['bucket1', 'bucket2'], если существуют директории bucket1 и bucket2
```

### `spacy_refine_chunks`

```python
def spacy_refine_chunks(source_iterator):
    """
    Использует spaCy для улучшения текстовых чанков из итератора.

    Args:
        source_iterator (Iterator[str]): Итератор, предоставляющий текстовые чанки.

    Yields:
        str: Улучшенные текстовые чанки.

    Raises:
        MissingRequirementsError: Если библиотека spaCy не установлена.
    """
```

**Назначение**: Улучшает текстовые фрагменты с использованием библиотеки `spaCy` для обработки естественного языка.

**Параметры**:
- `source_iterator` (Iterator[str]): Итератор, предоставляющий текстовые фрагменты для обработки.

**Возвращает**:
- `str`: Улучшенные текстовые фрагменты.

**Вызывает исключения**:
- `MissingRequirementsError`: Если библиотека `spaCy` не установлена.

**Как работает функция**:
1. Проверяет, установлена ли библиотека `spaCy`. Если нет, вызывает исключение `MissingRequirementsError`.
2. Загружает модель `en_core_web_sm` из `spaCy`.
3. Итерируется по текстовым фрагментам, предоставленным итератором `source_iterator`.
4. Для каждого фрагмента создает объект `doc` с использованием `nlp(page)`.
5. Извлекает предложения из документа `doc`.
6. Сортирует предложения по длине текста в обратном порядке и выбирает два самых длинных.
7. Выдает текст каждого выбранного предложения.

**Примеры**:
```python
# Пример использования:
# source_data = ["This is the first chunk.", "This is the second chunk."]
# refined_chunks = spacy_refine_chunks(source_data)
# for chunk in refined_chunks:
#     print(chunk)
```

### `get_filenames`

```python
def get_filenames(bucket_dir: Path):
    """
    Получает список имен файлов из файла FILE_LIST в указанной директории bucket.

    Args:
        bucket_dir (Path): Путь к директории bucket.

    Returns:
        list[str]: Список имен файлов.
    """
```

**Назначение**: Извлекает список имен файлов, хранящихся в файле `FILE_LIST` в указанном каталоге `bucket`.

**Параметры**:
- `bucket_dir` (Path): Путь к каталогу `bucket`.

**Возвращает**:
- `list[str]`: Список имен файлов, считанных из файла `FILE_LIST`.

**Как работает функция**:
1. Формирует путь к файлу `FILE_LIST` в каталоге `bucket`.
2. Проверяет, существует ли файл `FILE_LIST`.
3. Если файл существует, открывает его для чтения.
4. Считывает каждую строку из файла, удаляет лишние пробелы в начале и конце строки.
5. Возвращает список имен файлов.

**Примеры**:
```python
bucket_dir = Path("/path/to/bucket")
# Предположим, что файл FILE_LIST содержит:
# file1.txt
# file2.pdf
get_filenames(bucket_dir) # ['file1.txt', 'file2.pdf']
```

### `stream_read_files`

```python
def stream_read_files(bucket_dir: Path, filenames: list, delete_files: bool = False) -> Iterator[str]:
    """
    Считывает содержимое файлов из указанной директории bucket потоком.

    Args:
        bucket_dir (Path): Путь к директории bucket.
        filenames (list): Список имен файлов для чтения.
        delete_files (bool): Если True, удаляет файлы после прочтения. По умолчанию False.

    Yields:
        str: Содержимое файлов.
    """
```

**Назначение**: Читает содержимое файлов из указанного каталога `bucket` потоком, поддерживая различные типы файлов и возможность их удаления после прочтения.

**Параметры**:
- `bucket_dir` (Path): Путь к каталогу `bucket`.
- `filenames` (list): Список имен файлов для чтения.
- `delete_files` (bool): Если `True`, файлы будут удалены после прочтения. По умолчанию `False`.

**Возвращает**:
- `str`: Содержимое файлов.

**Как работает функция**:
1. Итерируется по списку имен файлов.
2. Для каждого файла проверяет его существование и размер (должен быть больше 0).
3. В зависимости от расширения файла использует соответствующую библиотеку для чтения содержимого:
   - `.zip`: извлекает все файлы и рекурсивно вызывает `stream_read_files` для каждого извлеченного файла. После обработки удаляет извлеченные файлы, если `delete_files` равен `True`.
   - `.pdf`: использует `PyPDF2`, `pdfplumber` или `pdfminer` для извлечения текста.
   - `.docx`: использует `docx` или `docx2txt` для извлечения текста.
   - `.odt`: использует `odfpy` для извлечения текста.
   - `.epub`: использует `ebooklib` для извлечения текста.
   - `.xlsx`: использует `pandas` для чтения данных и объединения ячеек в строку.
   - `.html`: использует `BeautifulSoup4` и `scrape_text` для извлечения текста.
   - Остальные файлы (текстовые): читает содержимое файла как текст.

**Примеры**:
```python
bucket_dir = Path("/path/to/bucket")
filenames = ["file1.txt", "file2.pdf", "archive.zip"]
# Содержимое file1.txt: "Hello, world!"
# Содержимое file2.pdf: "This is a PDF document."
# archive.zip содержит file3.txt с содержимым "Inside the archive."
# stream_read_files(bucket_dir, filenames) выдаст:
# "```file1.txt\nHello, world!\n```\n\n"
# "```file2.pdf\nThis is a PDF document.\n```\n\n"
# "```file3.txt\nInside the archive.\n```\n\n"
```

### `cache_stream`

```python
def cache_stream(stream: Iterator[str], bucket_dir: Path) -> Iterator[str]:
    """
    Кэширует содержимое потока в файл и возвращает поток с данными из кэша.

    Args:
        stream (Iterator[str]): Итератор, предоставляющий данные для кэширования.
        bucket_dir (Path): Путь к директории bucket, где будет сохранен кэш.

    Yields:
        str: Данные из потока.
    """
```

**Назначение**: Кэширует содержимое входного потока в файл для последующего использования и возвращает поток с данными из кэша.

**Параметры**:
- `stream` (Iterator[str]): Итератор, предоставляющий данные для кэширования.
- `bucket_dir` (Path): Путь к каталогу `bucket`, где будет сохранен кэш.

**Возвращает**:
- `str`: Данные из потока.

**Как работает функция**:
1. Определяет пути к файлу кэша и временному файлу.
2. Если файл кэша существует, читает его содержимое и выдает его.
3. Если файл кэша не существует, открывает временный файл для записи.
4. Итерируется по входному потоку, записывает каждый фрагмент во временный файл и выдает его.
5. После завершения итерации переименовывает временный файл в файл кэша.

**Примеры**:
```python
bucket_dir = Path("/path/to/bucket")
data_stream = iter(["Chunk 1", "Chunk 2", "Chunk 3"])
cached_stream = cache_stream(data_stream, bucket_dir)
for chunk in cached_stream:
    print(chunk)
```

### `is_complete`

```python
def is_complete(data: str):
    """
    Проверяет, является ли переданная строка завершенным блоком данных.

    Args:
        data (str): Строка данных для проверки.

    Returns:
        bool: True, если строка является завершенным блоком данных, иначе False.
    """
```

**Назначение**: Определяет, является ли переданная строка завершенным блоком данных на основе наличия маркеров начала и конца блока.

**Параметры**:
- `data` (str): Строка данных для проверки.

**Возвращает**:
- `bool`: `True`, если строка является завершенным блоком данных, и `False` в противном случае.

**Как работает функция**:
1. Проверяет, заканчивается ли строка подстрокой `"\\n\`\`\`\\n\\n"`.
2. Проверяет, четно ли количество подстрок "\`\`\`" в строке.
3. Возвращает `True`, если оба условия выполнены, и `False` в противном случае.

**Примеры**:
```python
is_complete("Some data\\n```\\n\\n") # True
is_complete("Some data\\n```\\n") # False
is_complete("```Some data\\n```\\n\\n") # False (количество ``` нечетное)
```

### `read_path_chunked`

```python
def read_path_chunked(path: Path):
    """
    Читает файл по частям, разделяя его на чанки определенного размера.

    Args:
        path (Path): Путь к файлу для чтения.

    Yields:
        str: Часть файла.
    """
```

**Назначение**: Читает файл по частям заданного размера, разделяя его на фрагменты и возвращая их в виде генератора.

**Параметры**:
- `path` (Path): Путь к файлу, который необходимо прочитать.

**Возвращает**:
- `str`: Часть файла.

**Как работает функция**:
1. Открывает файл для чтения в кодировке UTF-8.
2. Инициализирует переменные для отслеживания текущего размера фрагмента и буфера.
3. Итерируется по строкам файла.
4. Добавляет каждую строку в буфер и обновляет текущий размер фрагмента.
5. Если текущий размер фрагмента превышает 4096 байт, проверяет, является ли фрагмент завершенным (с помощью функции `is_complete`). Если фрагмент завершен или его размер превышает 8192 байта, выдает содержимое буфера и очищает буфер.
6. Если после завершения итерации в буфере остались данные, выдает их.

**Примеры**:
```python
file_path = Path("example.txt")
# Содержимое example.txt: "This is a small file."
# for chunk in read_path_chunked(file_path):
#     print(chunk) # Выведет: "This is a small file."
```

### `read_bucket`

```python
def read_bucket(bucket_dir: Path):
    """
    Читает содержимое файлов кэша из указанной директории bucket.

    Args:
        bucket_dir (Path): Путь к директории bucket.

    Yields:
        str: Содержимое файлов кэша.
    """
```

**Назначение**: Читает содержимое файлов кэша из указанного каталога bucket.

**Параметры**:
- `bucket_dir` (Path): Путь к каталогу bucket.

**Возвращает**:
- `str`: Содержимое файлов кэша.

**Как работает функция**:
1. Формирует пути к файлам кэша.
2. Проверяет наличие файлов кэша и читает их содержимое.
3. Если найден файл `spacy_file`, то отдает содержимое только его.
4. Если `spacy_file` не найден, но найден `cache_file`, то отдает содержимое `cache_file`.
5. Далее ищет файлы с префиксами `spacy_` и `plain_` с номерами от 0001 до 0999.
6. Если найден файл `spacy_{idx:04d}.cache`, то отдает содержимое только его.
7. Если найден файл `plain_{idx:04d}.cache`, то отдает содержимое только его.
8. Если ни один из файлов не найден, завершает работу.

**Примеры**:
```python
bucket_dir = Path("/path/to/bucket")
# Файлы в bucket_dir: cache_file, spacy_0001.cache, plain_0002.cache
# for chunk in read_bucket(bucket_dir):
#     print(chunk) # Выведет содержимое cache_file, spacy_0001.cache и plain_0002.cache
```

### `stream_read_parts_and_refine`

```python
def stream_read_parts_and_refine(bucket_dir: Path, delete_files: bool = False) -> Iterator[str]:
    """
    Читает и улучшает текстовые части из указанной директории bucket потоком.

    Args:
        bucket_dir (Path): Путь к директории bucket.
        delete_files (bool): Если True, удаляет файлы после прочтения. По умолчанию False.

    Yields:
        str: Улучшенные текстовые части.
    """
```

**Назначение**: Читает части файлов из указанного каталога bucket, улучшает их с помощью `spacy_refine_chunks` и возвращает улучшенные фрагменты потоком.

**Параметры**:
- `bucket_dir` (Path): Путь к каталогу bucket.
- `delete_files` (bool: Если `True`, удаляет части файлов после обработки. По умолчанию `False`.

**Возвращает**:
- `str`: Улучшенные текстовые части.

**Как работает функция**:
1. Формирует пути к файлам кэша и частям файлов.
2. Если файл `space_file` не существует, но существует `cache_file`, вызывает функцию `split_file_by_size_and_newline` для разделения `cache_file` на части.
3. Итерируется по частям файлов (с именами `plain_{idx:04d}.cache`, где `idx` от 1 до 999).
4. Для каждой части файла читает ее содержимое, улучшает с помощью `spacy_refine_chunks` и выдает улучшенные фрагменты.
5. После обработки каждой части файла переименовывает временный файл с улучшенными фрагментами в файл кэша (`spacy_{idx:04d}.cache`).
6. Если `delete_files` равен `True`, удаляет обработанную часть файла.

**Примеры**:
```python
bucket_dir = Path("/path/to/bucket")
# В bucket_dir существуют файлы: cache_file
# После вызова split_file_by_size_and_newline будут созданы plain_0001.cache, plain_0002.cache и т.д.
# for chunk in stream_read_parts_and_refine(bucket_dir, delete_files=True):
#     print(chunk) # Выведет улучшенные фрагменты из каждой части файла
```

### `split_file_by_size_and_newline`

```python
def split_file_by_size_and_newline(input_filename, output_dir, chunk_size_bytes=1024*1024): # 1MB
    """Splits a file into chunks of approximately chunk_size_bytes, splitting only at newline characters.

    Args:
        input_filename: Path to the input file.
        output_prefix: Prefix for the output files (e.g., 'output_part_').
        chunk_size_bytes: Desired size of each chunk in bytes.
    """
```

**Назначение**: Разделяет файл на части заданного размера, разделяя только по символам новой строки.

**Параметры**:
- `input_filename`: Путь к входному файлу.
- `output_dir`: Префикс для выходных файлов (например, 'output_part_').
- `chunk_size_bytes`: Желаемый размер каждой части в байтах.

**Как работает функция**:
1. Открывает входной файл для чтения в кодировке UTF-8.
2. Инициализирует переменные для отслеживания номера части, текущего содержимого части и ее размера.
3. Итерируется по строкам входного файла.
4. Добавляет каждую строку к текущему содержимому части и увеличивает размер части.
5. Если размер текущей части превышает заданный размер (`chunk_size_bytes`), записывает текущую часть в выходной файл, создает новый файл и сбрасывает переменные.
6. Записывает последнюю часть (если она не пуста) в выходной файл.

**Примеры**:
```python
input_file = "input.txt"
output_dir = "/path/to/output"
# После вызова split_file_by_size_and_newline(input_file, output_dir) будут созданы файлы output_0001.txt, output_0002.txt и т.д.
```

### `get_filename`

```python
async def get_filename(response: ClientResponse) -> str:
    """
    Attempts to extract a filename from an aiohttp response. Prioritizes Content-Disposition, then URL.

    Args:
        response: The aiohttp ClientResponse object.

    Returns:
        The filename as a string, or None if it cannot be determined.
    """
```

**Назначение**: Извлекает имя файла из ответа `aiohttp`, приоритезируя заголовок `Content-Disposition`, а затем URL.

**Параметры**:
- `response`: Объект `ClientResponse` из библиотеки `aiohttp`.

**Возвращает**:
- `str`: Имя файла в виде строки или `None`, если не удалось определить имя файла.

**Как работает функция**:
1. Пытается получить имя файла из заголовка `Content-Disposition`.
2. Если заголовок `Content-Disposition` отсутствует или не содержит имени файла, пытается извлечь имя файла из URL.
3. Если имя файла не удалось извлечь ни из заголовка, ни из URL, возвращает `None`.

**Примеры**:
```python
# response.headers['Content-Disposition'] = 'attachment; filename="example.pdf"'
# await get_filename(response) # Вернет: "example.pdf"
# response.url = "http://example.com/files/document.docx"
# await get_filename(response) # Вернет: "document.docx"
```

### `get_file_extension`

```python
async def get_file_extension(response: ClientResponse):
    """
    Attempts to determine the file extension from an aiohttp response.  Improved to handle more types.

    Args:
        response: The aiohttp ClientResponse object.

    Returns:
        The file extension (e.g., ".html", ".json", ".pdf", ".zip", ".md", ".txt") as a string,
        or None if it cannot be determined.
    """
```

**Назначение**: Определяет расширение файла из ответа `aiohttp`, анализируя заголовок `Content-Type` и URL.

**Параметры**:
- `response`: Объект `ClientResponse` из библиотеки `aiohttp`.

**Возвращает**:
- `str`: Расширение файла (например, ".html", ".json", ".pdf", ".zip", ".md", ".txt") в виде строки или `None`, если не удалось определить расширение.

**Как работает функция**:
1. Пытается определить расширение файла из заголовка `Content-Type`.
2. Если заголовок `Content-Type` отсутствует или не содержит информации о расширении файла, пытается извлечь расширение из URL.
3. Если расширение файла не удалось извлечь ни из заголовка, ни из URL, возвращает `None`.

**Примеры**:
```python
# response.headers['Content-Type'] = 'application/pdf'
# await get_file_extension(response) # Вернет: ".pdf"
# response.url = "http://example.com/files/document.docx"
# await get_file_extension(response) # Вернет: ".docx"
```

### `read_links`

```python
def read_links(html: str, base: str) -> set[str]:
    """
    Извлекает ссылки из HTML-кода, используя BeautifulSoup4.

    Args:
        html (str): HTML-код для извлечения ссылок.
        base (str): Базовый URL для объединения относительных ссылок.

    Returns:
        set[str]: Множество URL-адресов, найденных в HTML-коде.
    """
```

**Назначение**: Извлекает все ссылки из HTML-кода, используя BeautifulSoup4, и возвращает их в виде множества полных URL-адресов.

**Параметры**:
- `html` (str): HTML-код, из которого необходимо извлечь ссылки.
- `base` (str): Базовый URL-адрес для объединения относительных ссылок.

**Возвращает**:
- `set[str]`: Множество полных URL-адресов, найденных в HTML-коде.

**Как работает функция**:
1. Создает объект BeautifulSoup из HTML-кода.
2. Пытается найти основной контент, используя селекторы CSS.
3. Извлекает все ссылки (`<a>`) из основного контента.
4. Фильтрует ссылки, удаляя те, у которых есть атрибут `rel` со значением `nofollow`.
5. Объединяет относительные ссылки с базовым URL-адресом.
6. Возвращает множество полных URL-адресов.

**Примеры**:
```python
html_code = '<a href="https://example.com">Example</a><a href="/about">About</a>'
base_url = "https://example.com"
# read_links(html_code, base_url) # Вернет: {'https://example.com', 'https://example.com/about'}
```

### `download_urls`

```python
async def download_urls(
    bucket_dir: Path,
    urls: list[str],
    max_depth: int = 0,
    loading_urls: set[str] = set(),
    lock: asyncio.Lock = None,
    delay: int = 3,
    new_urls: list[str] = list(),
    group_size: int = 5,
    timeout: int = 10,
    proxy: Optional[str] = None
) -> AsyncIterator[str]:
    """
    Загружает файлы по списку URL-адресов асинхронно.

    Args:
        bucket_dir (Path): Путь к директории bucket, куда будут загружены файлы.
        urls (list[str]): Список URL-адресов для загрузки.
        max_depth (int): Максимальная глубина рекурсивной загрузки ссылок с HTML-страниц. По умолчанию 0 (не рекурсивно).
        loading_urls (set[str]): Множество URL-адресов, которые уже загружаются. Используется для предотвращения повторной загрузки.
        lock (asyncio.Lock): Блокировка asyncio для синхронизации доступа к общим ресурсам.
        delay (int): Задержка в секундах между запросами. По умолчанию 3 секунды.
        new_urls (list[str]): Список для добавления новых URL-адресов, найденных на загруженных HTML-страницах.
        group_size (int): Количество URL-адресов для одновременной загрузки. По умолчанию 5.
        timeout (int): Время ожидания ответа от сервера в секундах. По умолчанию 10 секунд.
        proxy (Optional[str]): Прокси-сервер для использования при загрузке файлов. По умолчанию None (без прокси).

    Yields:
        str: Имя файла, который был успешно загружен.
    """
```

**Назначение**: Асинхронно загружает файлы по списку URL-адресов, поддерживает рекурсивную загрузку ссылок с HTML-страниц и предотвращает повторную загрузку одних и тех же URL-адресов.

**Параметры**:
- `bucket_dir` (Path): Путь к каталогу `bucket`, куда будут загружены файлы.
- `urls` (list[str]): Список URL-адресов для загрузки.
- `max_depth` (int): Максимальная глубина рекурсивной загрузки ссылок с HTML-страниц. По умолчанию 0 (не рекурсивно).
- `loading_urls` (set[str]): Множество URL-адресов, которые уже загружаются. Используется для предотвращения повторной загрузки.
- `lock` (asyncio.Lock): Блокировка asyncio для синхронизации доступа к общим ресурсам.
- `delay` (int): Задержка в секундах между запросами. По умолчанию 3 секунды.
- `new_urls` (list[str]): Список для добавления новых URL-адресов, найденных на загруженных HTML-страницах.
- `group_size` (int): Количество URL-адресов для одновременной загрузки. По умолчанию 5.
- `timeout` (int): Время ожидания ответа от сервера в секундах. По умолчанию 10 секунд.
- `proxy` (Optional[str]): Прокси-сервер для использования при загрузке файлов. По умолчанию `None` (без прокси).

**Возвращает**:
- `str`: Имя файла, который был успешно загружен.

**Как работает функция**:
1. Инициализирует `ClientSession` с использованием `get_connector` для поддержки прокси и заданного времени ожидания.
2. Определяет внутреннюю асинхронную функцию `download_url`, которая загружает файл по заданному URL-адресу:
   - Получает ответ от сервера.
   - Извлекает имя файла из ответа, используя функцию `get_filename`.
   - Если имя файла не удалось получить или расширение файла не поддерживается, возвращает `None`.
   - Если расширение файла `.html` и `max_depth > 0`, извлекает ссылки из HTML-кода с помощью функции `read_links` и добавляет их в список `new_urls` для дальнейшей загрузки.
   - Сохраняет содержимое файла в указанном каталоге `bucket`.
   - Возвращает имя файла.
3. Использует `asyncio.gather` для одновременной загрузки нескольких URL-адресов.
4. После завершения загрузки всех URL-адресов из списка, рекурсивно вызывает себя для загрузки новых URL-адресов, найденных на загруженных HTML-страницах (если `max_depth > 0`).

**Примеры**:
```python
bucket_dir = Path("/path/to/bucket")
urls = ["http://example.com/file1.pdf", "http://example.com/file2.txt"]
# async for filename in download_urls(bucket_dir, urls, max_depth=1):
#     print(filename) # Выведет имена загруженных файлов
```

### `get_downloads_urls`

```python
def get_downloads_urls(bucket_dir: Path, delete_files: bool = False) -> Iterator[str]:
    """
    Получает список URL-адресов для загрузки из файла DOWNLOADS_FILE в указанной директории bucket.

    Args:
        bucket_dir (Path): Путь к директории bucket.
        delete_files (bool): Если True, удаляет файл DOWNLOADS_FILE после прочтения. По умолчанию False.

    Yields:
        dict: Словарь с информацией о URL-адресе (или URL-адресах) и дополнительных параметрах.
    """
```

**Назначение**: Получает список URL-адресов для загрузки из файла `DOWNLOADS_FILE` в указанном каталоге `bucket`.

**Параметры**:
- `bucket_dir` (Path): Путь к каталогу `bucket`.
- `delete_files` (bool): Если `True`, удаляет файл `DOWNLOADS_FILE` после прочтения. По умолчанию `False`.

**Возвращает**:
- `dict`: Словарь с информацией о URL-адресе (или URL-адресах) и дополнительных параметрах.

**Как работает функция**:
1. Формирует путь к файлу `DOWNLOADS_FILE` в каталоге `bucket`.
2. Проверяет, существует ли файл `DOWNLOADS_FILE`.
3. Если файл существует, открывает его для чтения.
4. Загружает данные из файла в формате JSON.
5. Если `delete_files` равен `True`, удаляет файл `DOWNLOADS_FILE`.
6. Итерируется по списку элементов в загруженных данных и возвращает каждый элемент, содержащий URL-адрес (или URL-адреса).

**Примеры**:
```python
bucket_dir = Path("/path/to/bucket")
# Файл DOWNLOADS_FILE содержит:
# [{"url":