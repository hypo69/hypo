# Модуль crawlee_python

## Обзор

Этот модуль предоставляет класс `CrawleePython` для запуска веб-скрапинга с использованием Playwright.  Он позволяет задать максимальное количество запросов, режим работы (с графическим интерфейсом или без него) и тип браузера. Модуль умеет получать данные с сайта, сохранять их в JSON-файл и выводить отслеженные URL-адреса.

## Оглавление

- [Модуль crawlee_python](#модуль-crawlee_python)
- [Обзор](#обзор)
- [Класс CrawleePython](#класс-crawlee_python)
    - [`__init__`](#__init__)
    - [`setup_crawler`](#setup_crawler)
    - [`run_crawler`](#run_crawler)
    - [`export_data`](#export_data)
    - [`get_data`](#get_data)
    - [`run`](#run)
- [Пример использования](#пример-использования)


## Класс `CrawleePython`

### `__init__`

**Описание**: Инициализирует экземпляр класса с параметрами для создания и управления PlaywrightCrawler.

**Параметры**:

- `max_requests` (int, опционально): Максимальное количество запросов, которые могут быть обработаны за один цикл. По умолчанию 5.
- `headless` (bool, опционально): Флаг, определяющий, запускать браузер с графическим интерфейсом или без. По умолчанию `False`.
- `browser_type` (str, опционально): Тип браузера для использования. По умолчанию `'firefox'`.


### `setup_crawler`

**Описание**: Настраивает экземпляр `PlaywrightCrawler`.

**Возвращает**:
    -  `None`


### `run_crawler`

**Описание**: Запускает скрапинг для заданного списка URL-адресов.

**Параметры**:

- `urls` (list[str]): Список URL-адресов для начала скрапинга.

**Возвращает**:
    - `None`


### `export_data`

**Описание**: Экспортирует все собранные данные в JSON-файл.

**Параметры**:

- `file_path` (str): Путь к файлу, в который будут сохранены данные.


**Возвращает**:
    - `None`


### `get_data`

**Описание**: Возвращает собранные данные.

**Возвращает**:
    - `dict`: Словарь с собранными данными.


### `run`

**Описание**: Основной метод для выполнения всего процесса скрапинга, включая настройку, запуск и экспорт данных.

**Параметры**:

- `urls` (list[str]): Список URL-адресов для начала скрапинга.

**Возвращает**:
    - `None`


## Пример использования

```python
import asyncio
from hypotez.src.webdriver.crawlee_python.crawlee_python import CrawleePython
import os
import sys

async def main():
    experiment = CrawleePython(max_requests=5, headless=False, browser_type='firefox')
    await experiment.run(['https://ksp.co.il'])


if __name__ == '__main__':
    asyncio.run(main())
```

Этот пример показывает, как создать экземпляр `CrawleePython`, указать URL-адреса для начала скрапинга и запустить процесс. Обратите внимание, что для работы примера необходимо импортировать класс `CrawleePython` и модуль `asyncio`.  Также добавлен блок `if __name__ == '__main__':`, чтобы гарантировать, что функция `main` будет вызвана только при непосредственном запуске скрипта.