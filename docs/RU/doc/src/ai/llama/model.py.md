# Модуль `model.py`

## Обзор

Модуль `model.py` предназначен для загрузки и использования языковой модели Llama 3.1 с помощью библиотеки `llama-cpp-python`. Он демонстрирует базовое использование модели, загружая её из репозитория Hugging Face и генерируя текст.

## Оглавление

1. [Обзор](#обзор)
2. [Импорты](#импорты)
3. [Переменные](#переменные)
4. [Пример использования](#пример-использования)

## Импорты

Модуль использует следующий импорт:

- `llama_cpp.Llama`: Класс для работы с языковыми моделями Llama.

## Переменные

### `llm`

- **Описание**: Экземпляр модели Llama, загруженной из репозитория Hugging Face.
- **Тип**: `llama_cpp.Llama`
- **Инициализация**: Модель загружается с использованием `Llama.from_pretrained()`, указывая репозиторий и имя файла модели.
- **Значение**: `Llama.from_pretrained(repo_id="lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF", filename="Meta-Llama-3.1-8B-Instruct-IQ4_XS.gguf")`

## Пример использования

### Генерация текста

- **Описание**: Пример использования модели для генерации текста на основе заданной начальной фразы.
- **Код**:
```python
output = llm(
    "Once upon a time,",
    max_tokens=512,
    echo=True
)
print(output)
```
- **Параметры**:
  - `prompt` (str): Начальная фраза для генерации текста. В данном примере `"Once upon a time,"`.
  - `max_tokens` (int): Максимальное количество токенов в сгенерированном тексте. В данном примере `512`.
  - `echo` (bool): Флаг, указывающий, нужно ли включать начальную фразу в результат. В данном примере `True`.
- **Вывод**: Результат генерации текста выводится на консоль с помощью `print()`.