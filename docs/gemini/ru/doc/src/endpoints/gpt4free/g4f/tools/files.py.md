# Модуль для работы с файлами
=================================================

Модуль содержит функции для обработки различных типов файлов, включая `pdf`, `docx`, `odt`, `epub`, `xlsx`, `html`, `zip`, а также для скачивания файлов из интернета и извлечения текста из них.

## Обзор

Модуль предоставляет инструменты для работы с файлами разных форматов, их скачивания, обработки и извлечения текстового содержимого. Он также включает функциональность для кэширования и обработки текста с использованием `spacy`. Модуль разбит на несколько основных блоков:

1.  **Поддержка различных форматов файлов**: Обеспечивает чтение и извлечение текста из `pdf`, `docx`, `odt`, `epub`, `xlsx`, `html` и `zip` файлов.

2.  **Скачивание файлов из интернета**: Позволяет скачивать файлы по `URL` и сохранять их в указанной директории.

3.  **Кэширование**: Включает механизм кэширования для оптимизации повторного чтения файлов.

4.  **Обработка текста с использованием `spacy`**: Предоставляет возможность улучшить качество извлеченного текста с помощью библиотеки `spacy`.

5.  **Управление директориями и файлами**: Функции для создания, проверки и удаления директорий и файлов.

## Подробнее

Этот модуль используется для обработки файлов, загруженных пользователем или скачанных из интернета. Он позволяет извлекать текстовое содержимое, которое затем может быть использовано для дальнейшей обработки, например, для анализа текста или построения поисковых индексов.

## Функции

### `secure_filename`

```python
def secure_filename(filename: str) -> str:
    """
    Очищает имя файла, удаляя небезопасные символы.

    Args:
        filename (str): Имя файла для очистки.

    Returns:
        str: Очищенное имя файла.
    """
```

**Назначение**:
Функция `secure_filename` принимает имя файла в качестве аргумента и возвращает его безопасную версию, удаляя или заменяя символы, которые могут быть опасными или нежелательными в контексте файловой системы.

**Параметры**:
-   `filename` (str): Имя файла, которое необходимо очистить.

**Возвращает**:
-   `str`: Безопасное имя файла, в котором все небезопасные символы заменены на безопасные.

**Как работает функция**:

1.  Проверяет, является ли входное имя файла `None`. Если это так, функция возвращает `None`.
2.  Использует регулярное выражение для замены всех символов, которые не являются буквенно-цифровыми, символами подчеркивания, точками, запятыми, плюсами или минусами, на символы подчеркивания.
3.  Удаляет все пробельные символы в начале и конце имени файла.
4.  Обрезает имя файла до 100 символов.
5.  Удаляет символы точек, запятых, подчеркиваний, плюсов и минусов из начала и конца имени файла.
6.  Возвращает очищенное имя файла.

**Примеры**:

```python
secure_filename("example file.txt")  # Возвращает "example_file.txt"
secure_filename("file@with#unsafe$chars.txt")  # Возвращает "file_with_unsafe_chars.txt"
```

### `supports_filename`

```python
def supports_filename(filename: str) -> bool:
    """
    Проверяет, поддерживается ли файл для обработки на основе его расширения и установленных библиотек.

    Args:
        filename (str): Имя файла для проверки.

    Returns:
        bool: `True`, если файл поддерживается, иначе `False`.

    Raises:
        MissingRequirementsError: Если необходимые библиотеки не установлены.
    """
```

**Назначение**:
Функция `supports_filename` проверяет, может ли файл быть обработан на основе его расширения и наличия необходимых библиотек.

**Параметры**:
-   `filename` (str): Имя файла, которое необходимо проверить.

**Возвращает**:
-   `bool`: Возвращает `True`, если файл поддерживается для обработки, и `False` в противном случае.

**Вызывает исключения**:
-   `MissingRequirementsError`: Вызывается, если для обработки файла требуются отсутствующие библиотеки.

**Как работает функция**:

1.  Проверяет расширение файла и наличие необходимых библиотек (`pypdf2`, `pdfplumber`, `pdfminer`, `docx`, `docx2txt`, `odfpy`, `ebooklib`, `openpyxl`, `beautifulsoup4`).
2.  Если расширение файла `.pdf`, проверяет наличие установленных библиотек для работы с `pdf` (`pypdf2`, `pdfplumber`, `pdfminer`). Если ни одна из библиотек не установлена, вызывает исключение `MissingRequirementsError`.
3.  Аналогично проверяет поддержку файлов `.docx`, `.odt`, `.epub`, `.xlsx`, `.html`.
4.  Для файлов `.zip` всегда возвращает `True`.
5.  Для файлов `package-lock.json` возвращает `False`.
6.  Для файлов с другими расширениями проверяет, входит ли расширение в список `PLAIN_FILE_EXTENSIONS`.
7.  Возвращает `True`, если файл поддерживается, и `False` в противном случае.

**Примеры**:

```python
supports_filename("example.pdf")  # Возвращает True, если установлена хотя бы одна из библиотек pypdf2, pdfplumber, pdfminer
supports_filename("example.docx")  # Возвращает True, если установлена хотя бы одна из библиотек docx, docx2txt
```

### `get_bucket_dir`

```python
def get_bucket_dir(*parts) -> str:
    """
    Возвращает путь к директории бакета, объединяя части пути и очищая имена файлов.

    Args:
        *parts: Части пути для объединения.

    Returns:
        str: Путь к директории бакета.
    """
```

**Назначение**:
Функция `get_bucket_dir` формирует путь к директории, используемой для хранения файлов, связанных с определенным "бакетом". Она объединяет переданные части пути, очищая каждую часть с помощью функции `secure_filename` для обеспечения безопасности файловой системы.

**Параметры**:
-   `*parts`: Неопределенное количество частей пути, которые нужно объединить.

**Возвращает**:
-   `str`: Полный путь к директории бакета.

**Как работает функция**:

1.  Получает путь к директории `cookies`, где хранятся данные о бакетах.
2.  Объединяет базовую директорию с переданными частями пути, очищая каждую часть пути с помощью функции `secure_filename`.
3.  Возвращает полученный путь.

**Примеры**:

```python
get_bucket_dir("user123", "file_uploads")  # Возвращает путь к директории бакета "user123/file_uploads"
```

### `get_buckets`

```python
def get_buckets() -> Optional[list[str]]:
    """
    Возвращает список директорий бакетов.

    Returns:
        Optional[list[str]]: Список директорий бакетов или `None`, если директория не существует.
    """
```

**Назначение**:
Функция `get_buckets` возвращает список директорий, которые находятся в директории `buckets`. Каждая директория представляет собой отдельный "бакет", используемый для хранения файлов.

**Возвращает**:
-   `Optional[list[str]]`: Список имен директорий бакетов. Возвращает `None`, если директория бакетов не существует или возникает ошибка при чтении директории.

**Как работает функция**:

1.  Формирует путь к директории `buckets`, где хранятся все бакеты.
2.  Пытается получить список всех директорий в директории `buckets`.
3.  Возвращает список директорий бакетов.
4.  В случае ошибки (например, директория не существует) возвращает `None`.

**Примеры**:

```python
get_buckets()  # Возвращает список ["bucket1", "bucket2"], если существуют директории "bucket1" и "bucket2"
```

### `spacy_refine_chunks`

```python
def spacy_refine_chunks(source_iterator) -> Iterator[str]:
    """
    Использует `spacy` для улучшения качества текста, разбивая его на предложения.

    Args:
        source_iterator: Итератор строк для обработки.

    Yields:
        str: Улучшенный текст.

    Raises:
        MissingRequirementsError: Если библиотека `spacy` не установлена.
    """
```

**Назначение**:
Функция `spacy_refine_chunks` использует библиотеку `spacy` для улучшения качества текста, поступающего из итератора `source_iterator`. Она разбивает текст на предложения и возвращает наиболее значимые из них.

**Параметры**:
-   `source_iterator`: Итератор, предоставляющий строки текста для обработки.

**Yields**:
-   `str`: Улучшенный текст, разбитый на предложения.

**Вызывает исключения**:
-   `MissingRequirementsError`: Вызывается, если библиотека `spacy` не установлена.

**Как работает функция**:

1.  Проверяет, установлена ли библиотека `spacy`. Если нет, вызывает исключение `MissingRequirementsError`.
2.  Загружает модель `spacy` ("en\_core\_web\_sm").
3.  Проходит по каждой странице текста, предоставляемой итератором `source_iterator`.
4.  Обрабатывает страницу текста с помощью модели `spacy`.
5.  Извлекает предложения из обработанного текста.
6.  Сортирует предложения по длине в обратном порядке и выбирает два самых длинных предложения.
7.  Возвращает каждое выбранное предложение.

**Примеры**:

```python
def text_generator():
    yield "This is the first sentence. This is the second sentence."
    yield "Another sentence here."

refined_chunks = spacy_refine_chunks(text_generator())
for chunk in refined_chunks:
    print(chunk)
```

### `get_filenames`

```python
def get_filenames(bucket_dir: Path) -> list[str]:
    """
    Получает список имен файлов из файла `FILE_LIST` в директории бакета.

    Args:
        bucket_dir (Path): Путь к директории бакета.

    Returns:
        list[str]: Список имен файлов.
    """
```

**Назначение**:
Функция `get_filenames` получает список имен файлов, сохраненных в файле `FILE_LIST` в указанной директории бакета.

**Параметры**:
-   `bucket_dir` (Path): Объект `Path`, представляющий путь к директории бакета.

**Возвращает**:
-   `list[str]`: Список имен файлов, прочитанных из файла `FILE_LIST`. Если файл не существует, возвращается пустой список.

**Как работает функция**:

1.  Формирует путь к файлу `FILE_LIST` в указанной директории бакета.
2.  Проверяет, существует ли файл.
3.  Если файл существует, открывает его для чтения и читает все строки.
4.  Удаляет пробельные символы в начале и конце каждой строки и возвращает список имен файлов.
5.  Если файл не существует, возвращает пустой список.

**Примеры**:

```python
bucket_dir = Path("/path/to/bucket")
filenames = get_filenames(bucket_dir)
print(filenames)
```

### `stream_read_files`

```python
def stream_read_files(bucket_dir: Path, filenames: list, delete_files: bool = False) -> Iterator[str]:
    """
    Читает содержимое файлов из директории бакета, поддерживая различные форматы файлов.

    Args:
        bucket_dir (Path): Путь к директории бакета.
        filenames (list): Список имен файлов для чтения.
        delete_files (bool): Если `True`, файлы будут удалены после прочтения.

    Yields:
        str: Содержимое файла.
    """
```

**Назначение**:
Функция `stream_read_files` читает содержимое файлов из указанной директории бакета, поддерживая различные форматы файлов, такие как `zip`, `pdf`, `docx`, `odt`, `epub`, `xlsx`, `html` и текстовые файлы.

**Параметры**:
-   `bucket_dir` (Path): Объект `Path`, представляющий путь к директории бакета.
-   `filenames` (list): Список имен файлов, которые необходимо прочитать.
-   `delete_files` (bool): Если установлено в `True`, файлы будут удалены после прочтения. По умолчанию `False`.

**Yields**:
-   `str`: Текстовое содержимое каждого файла.

**Как работает функция**:

1.  Проходит по списку имен файлов.
2.  Формирует полный путь к каждому файлу.
3.  Проверяет, существует ли файл и не является ли его размер нулевым. Если файл не существует или его размер равен нулю, переходит к следующему файлу.
4.  Если файл является `zip`-архивом:
    *   Извлекает все файлы из архива в директорию бакета.
    *   Рекурсивно вызывает `stream_read_files` для чтения извлеченных файлов.
    *   После обработки удаляет извлеченные файлы, если `delete_files` равно `True`.
5.  Для каждого файла формирует строку с именем файла в формате Markdown code block.
6.  В зависимости от расширения файла использует соответствующие библиотеки для чтения содержимого:
    *   `pdf`: `PyPDF2`, `pdfplumber`, `pdfminer`
    *   `docx`: `docx`, `docx2txt`
    *   `odt`: `odfpy`
    *   `epub`: `ebooklib`
    *   `xlsx`: `pandas`
    *   `html`: `beautifulsoup4`
    *   Текстовые файлы: чтение содержимого файла.
7.  Возвращает содержимое каждого файла.
8.  После каждого файла добавляет разделитель `\n\`\`\`\n\n`.

**Примеры**:

```python
bucket_dir = Path("/path/to/bucket")
filenames = ["example.pdf", "example.docx", "example.txt"]
for content in stream_read_files(bucket_dir, filenames, delete_files=True):
    print(content)
```

### `cache_stream`

```python
def cache_stream(stream: Iterator[str], bucket_dir: Path) -> Iterator[str]:
    """
    Кэширует содержимое потока в файл.

    Args:
        stream (Iterator[str]): Поток данных для кэширования.
        bucket_dir (Path): Директория для хранения кэша.

    Yields:
        str: Часть содержимого потока.
    """
```

**Назначение**:
Функция `cache_stream` принимает поток данных в виде итератора строк, кэширует его содержимое в файл и возвращает этот поток.

**Параметры**:
-   `stream` (Iterator[str]): Итератор, предоставляющий строки текста для кэширования.
-   `bucket_dir` (Path): Объект `Path`, представляющий директорию, в которой будет сохранен кэш.

**Yields**:
-   `str`: Каждая часть содержимого потока.

**Как работает функция**:

1.  Формирует путь к файлу кэша (`PLAIN_CACHE`) в указанной директории.
2.  Формирует путь к временному файлу, который будет использоваться для записи данных.
3.  Проверяет, существует ли файл кэша. Если существует, читает его содержимое и возвращает его.
4.  Если файл кэша не существует, открывает временный файл для записи.
5.  Проходит по каждой части содержимого потока, записывает ее во временный файл и возвращает ее.
6.  После записи всего содержимого потока переименовывает временный файл в файл кэша.

**Примеры**:

```python
def data_stream():
    yield "This is the first chunk."
    yield "This is the second chunk."

bucket_dir = Path("/path/to/bucket")
cached_stream = cache_stream(data_stream(), bucket_dir)
for chunk in cached_stream:
    print(chunk)
```

### `is_complete`

```python
def is_complete(data: str) -> bool:
    """
    Проверяет, является ли строка завершенным блоком текста.

    Args:
        data (str): Строка для проверки.

    Returns:
        bool: `True`, если строка завершена, иначе `False`.
    """
```

**Назначение**:
Функция `is_complete` проверяет, является ли переданная строка завершенным блоком текста, основываясь на наличии маркеров начала и конца блока (`\n\`\`\`\n\n`) и четном количестве маркеров code block (`\````).

**Параметры**:
-   `data` (str): Строка, которую необходимо проверить.

**Возвращает**:
-   `bool`: `True`, если строка завершена, и `False` в противном случае.

**Как работает функция**:

1.  Проверяет, заканчивается ли строка маркером конца блока (`\n\`\`\`\n\n`).
2.  Подсчитывает количество маркеров code block (`\````) в строке.
3.  Проверяет, является ли количество маркеров code block четным.
4.  Возвращает `True`, если строка заканчивается маркером конца блока и количество маркеров code block четное, и `False` в противном случае.

**Примеры**:

```python
is_complete("Some text\n```\n\n")  # Возвращает True
is_complete("Some text\n```")  # Возвращает False
is_complete("```Some text\n```\n```\n\n")  # Возвращает False (нечетное количество ```)
```

### `read_path_chunked`

```python
def read_path_chunked(path: Path) -> Iterator[str]:
    """
    Читает файл по частям, возвращая содержимое в виде итератора.

    Args:
        path (Path): Путь к файлу.

    Yields:
        str: Часть содержимого файла.
    """
```

**Назначение**:
Функция `read_path_chunked` читает файл по частям (чанками) и возвращает содержимое в виде итератора.

**Параметры**:
-   `path` (Path): Объект `Path`, представляющий путь к файлу.

**Yields**:
-   `str`: Часть содержимого файла.

**Как работает функция**:

1.  Открывает файл для чтения в кодировке `utf-8`.
2.  Инициализирует переменные для хранения текущего размера чанка и буфера.
3.  Проходит по каждой строке файла.
4.  Увеличивает текущий размер чанка на длину строки в байтах.
5.  Добавляет строку в буфер.
6.  Если текущий размер чанка превышает 4096 байт, проверяет, является ли буфер завершенным блоком текста (с помощью функции `is_complete`). Если буфер завершен или текущий размер чанка превышает 8192 байта, возвращает содержимое буфера и очищает буфер и текущий размер чанка.
7.  После обработки всех строк файла, если в буфере осталось содержимое, возвращает его.

**Примеры**:

```python
file_path = Path("/path/to/file.txt")
for chunk in read_path_chunked(file_path):
    print(chunk)
```

### `read_bucket`

```python
def read_bucket(bucket_dir: Path) -> Iterator[str]:
    """
    Читает содержимое кэшированных файлов из директории бакета.

    Args:
        bucket_dir (Path): Путь к директории бакета.

    Yields:
        str: Содержимое кэшированного файла.
    """
```

**Назначение**:
Функция `read_bucket` читает содержимое кэшированных файлов из указанной директории бакета и возвращает его в виде итератора.

**Параметры**:
-   `bucket_dir` (Path): Объект `Path`, представляющий путь к директории бакета.

**Yields**:
-   `str`: Содержимое кэшированного файла.

**Как работает функция**:

1.  Формирует пути к файлам кэша (`PLAIN_CACHE`, `spacy_XXXX.cache`, `plain_XXXX.cache`) в указанной директории бакета.
2.  Если существует файл `PLAIN_CACHE`, возвращает его содержимое.
3.  Проходит по файлам `spacy_XXXX.cache` и `plain_XXXX.cache` с индексами от 1 до 999.
4.  Если существует файл `spacy_XXXX.cache`, возвращает его содержимое.
5.  Если существует файл `plain_XXXX.cache`, возвращает его содержимое.
6.  Если ни один из файлов не существует, завершает работу.

**Примеры**:

```python
bucket_dir = Path("/path/to/bucket")
for content in read_bucket(bucket_dir):
    print(content)
```

### `stream_read_parts_and_refine`

```python
def stream_read_parts_and_refine(bucket_dir: Path, delete_files: bool = False) -> Iterator[str]:
    """
    Читает части файлов из директории бакета и улучшает их с помощью `spacy`.

    Args:
        bucket_dir (Path): Путь к директории бакета.
        delete_files (bool): Если `True`, файлы будут удалены после прочтения.

    Yields:
        str: Улучшенный текст.
    """
```

**Назначение**:
Функция `stream_read_parts_and_refine` читает части файлов из указанной директории бакета, улучшает их с помощью библиотеки `spacy` и возвращает улучшенный текст в виде итератора.

**Параметры**:
-   `bucket_dir` (Path): Объект `Path`, представляющий путь к директории бакета.
-   `delete_files` (bool): Если установлено в `True`, файлы будут удалены после прочтения. По умолчанию `False`.

**Yields**:
-   `str`: Улучшенный текст.

**Как работает функция**:

1.  Формирует пути к файлам кэша (`PLAIN_CACHE`, `spacy_XXXX.cache`, `plain_XXXX.cache`) в указанной директории бакета.
2.  Если существует файл `PLAIN_CACHE` и не существуют файлы `spacy_0001.cache` и `plain_0001.cache`, разделяет файл `PLAIN_CACHE` на части с помощью функции `split_file_by_size_and_newline`.
3.  Проходит по файлам `plain_XXXX.cache` с индексами от 1 до 999.
4.  Формирует пути к временному файлу и файлу кэша `spacy_XXXX.cache`.
5.  Если существует файл `spacy_XXXX.cache`, читает его содержимое и возвращает его.
6.  Если не существует файл `plain_XXXX.cache`, завершает работу.
7.  Открывает временный файл для записи.
8.  Проходит по частям файла `plain_XXXX.cache` с помощью функции `read_path_chunked` и улучшает их с помощью функции `spacy_refine_chunks`.
9.  Записывает улучшенный текст во временный файл и возвращает его.
10. Переименовывает временный файл в файл кэша `spacy_XXXX.cache`.
11. Если `delete_files` равно `True`, удаляет файл `plain_XXXX.cache`.

**Примеры**:

```python
bucket_dir = Path("/path/to/bucket")
for content in stream_read_parts_and_refine(bucket_dir, delete_files=True):
    print(content)
```

### `split_file_by_size_and_newline`

```python
def split_file_by_size_and_newline(input_filename, output_dir, chunk_size_bytes=1024*1024): # 1MB
    """Splits a file into chunks of approximately chunk_size_bytes, splitting only at newline characters.

    Args:
        input_filename: Path to the input file.
        output_prefix: Prefix for the output files (e.g., 'output_part_').
        chunk_size_bytes: Desired size of each chunk in bytes.
    """
```

**Назначение**:
Функция `split_file_by_size_and_newline` разделяет входной файл на части (чанки) приблизительного размера `chunk_size_bytes`, разделяя файл только по символам новой строки.

**Параметры**:
-   `input_filename`: Путь к входному файлу.
-   `output_dir`: Директория для сохранения выходных файлов.
-   `chunk_size_bytes`: Желаемый размер каждой части в байтах. По умолчанию 1MB.

**Как работает функция**:

1.  Формирует префикс для выходных файлов на основе имени входного файла и директории.
2.  Открывает входной файл для чтения в кодировке `utf-8`.
3.  Инициализирует переменные для хранения номера части, текущего чанка и его размера.
4.  Проходит по каждой строке входного файла.
5.  Добавляет строку в текущий чанк и увеличивает размер чанка на длину строки в байтах.
6.  Если размер чанка превышает `chunk_size_bytes`, проверяет, является ли текущий чанк завершенным блоком текста (с помощью функции `is_complete`). Если чанк завершен или размер чанка превышает `chunk_size_bytes * 2`, сохраняет текущий чанк в отдельный файл с именем `output_prefix + chunk_num + split_filename[1]`, очищает текущий чанк и его размер, увеличивает номер части.
7.  После обработки всех строк входного файла, если в текущем чанке осталось содержимое, сохраняет его в отдельный файл.

**Примеры**:

```python
input_filename = "/path/to/input.txt"
output_dir = "/path/to/output"
split_file_by_size_and_newline(input_filename, output_dir)
```

### `get_filename`

```python
async def get_filename(response: ClientResponse) -> str:
    """
    Attempts to extract a filename from an aiohttp response. Prioritizes Content-Disposition, then URL.

    Args:
        response: The aiohttp ClientResponse object.

    Returns:
        The filename as a string, or None if it cannot be determined.
    """
```

**Назначение**:
Асинхронная функция `get_filename` пытается извлечь имя файла из ответа `aiohttp`. Приоритет отдается заголовку `Content-Disposition`, затем `URL`.

**Параметры**:
-   `response`: Объект `ClientResponse` из библиотеки `aiohttp`.

**Возвращает**:
-   `str`: Имя файла в виде строки, или `None`, если имя файла не может быть определено.

**Как работает функция**:

1.  Пытается получить значение заголовка `Content-Disposition` из ответа. Если заголовок существует, пытается извлечь имя файла из него.
2.  Если имя файла успешно извлечено из `Content-Disposition`, возвращает его, предварительно обработав с помощью функции `secure_filename`.
3.  Если заголовок `Content-Disposition` отсутствует или не содержит имени файла, пытается получить тип контента и `URL` из ответа.
4.  Если тип контента и `URL` существуют, пытается определить расширение файла с помощью функции `get_file_extension`.
5.  Если расширение файла определено, формирует имя файла на основе `URL`, хеша `URL` и расширения файла.
6.  Возвращает сформированное имя файла или `None`, если имя файла не может быть определено.

**Примеры**:

```python
async def main():
    async with ClientSession() as session:
        async with session.get("https://example.com/file.pdf") as response:
            filename = await get_filename(response)
            print(filename)

asyncio.run(main())
```

### `get_file_extension`

```python
async def get_file_extension(response: ClientResponse):
    """
    Attempts to determine the file extension from an aiohttp response.  Improved to handle more types.

    Args:
        response: The aiohttp ClientResponse object.

    Returns:
        The file extension (e.g., ".html", ".json", ".pdf", ".zip", ".md", ".txt") as a string,
        or None if it cannot be determined.
    """
```

**Назначение**:
Асинхронная функция `get_file_extension` пытается определить расширение файла из ответа `aiohttp`.

**Параметры**:
-   `response`: Объект `ClientResponse` из библиотеки `aiohttp`.

**Возвращает**:
-   `str`: Расширение файла (например, ".html", ".json", ".pdf", ".zip", ".md", ".txt") в виде строки, или `None`, если расширение не может быть определено.

**Как работает функция**:

1.  Пытается получить значение заголовка `Content-Type` из ответа.
2.  Если заголовок `Content-Type` существует, определяет расширение файла на основе типа контента (например, "html", "json", "pdf", "zip", "text/plain", "markdown").
3.  Если заголовок `Content-Type` отсутствует или не содержит информации о типе файла, пытается определить расширение файла на основе `URL`.
4.  Возвращает определенное расширение файла или `None`, если расширение не может быть определено.

**Примеры**:

```python
async def main():
    async with ClientSession() as session:
        async with session.get("https://example.com/file.pdf") as response:
            extension = await get_file_extension(response)
            print(extension)

asyncio.run(main())
```

### `read_links`

```python
def read_links(html: str, base: str) -> set[str]:
    """
    Извлекает ссылки из HTML-кода, используя BeautifulSoup.

    Args:
        html (str): HTML-код для анализа.
        base (str): Базовый URL для объединения относительных ссылок.

    Returns:
        set[str]: Множество URL-адресов.
    """
```

**Назначение**:
Функция `read_links` извлекает ссылки из предоставленного `HTML`-кода и возвращает их в виде множества `URL`-адресов.

**Параметры**:
-   `html` (str): `HTML`-код, из которого необходимо извлечь ссылки.
-   `base` (str): Базовый `URL`, который используется для объединения относительных ссылок в абсолютные.

**Возвращает**:
-   `set[str]`: Множество `URL`-адресов, извлеченных из `HTML`-кода.

**Как работает функция**:

1.  Использует `BeautifulSoup` для анализа `HTML`-кода.
2.  Пытается найти основной контент, выбирая элементы с определенными селекторами (`main`, `.main-content-wrapper`, `.main-content` и т. д.). Если элемент найден, дальнейший поиск ссылок производится только в этом элементе.
3.  Ищет все элементы `a` (ссылки) в `HTML`-коде.
4.  Проверяет, есть ли у ссылки атрибут `rel` и содержит ли он значение `nofollow`. Если есть, ссылка пропускается.
5.  Извлекает значение атрибута `href` из каждой ссылки.
6.  Проверяет, начинается ли `URL` с `https://` или `/`.
7.  Объединяет относительные `URL` с базовым `URL` с помощью `urllib.parse.urljoin`.
8.  Возвращает множество уникальных `URL`-адресов.

**Примеры**:

```python
html = '<a href="https://example.com">Example</a><a href="/about">About</a>'
base = "https://example.com"
links = read_links(html, base)
print(links)  # Возвращает {'https://example.com', 'https://example.com/about'}
```

### `download_urls`

```python
async def download_urls(
    bucket_dir: Path,
    urls: list[str],
    max_depth: int = 0,
    loading_urls: set[str] = set(),
    lock: asyncio.Lock = None,
    delay: int = 3,
    new_urls: list[str] = list(),
    group_size: int = 5,
    timeout: int = 10,
    proxy: Optional[str] = None
) -> AsyncIterator[str]:
    """
    Асинхронно скачивает файлы по URL-адресам и сохраняет их в директории бакета.

    Args:
        bucket_dir (Path): Путь к директории бакета.
        urls (list[str]): Список URL-адресов для скачивания.
        max_depth (int): Максимальная глубина рекурсивного скачивания HTML-страниц.
        loading_urls (set[str]): Множество URL-адресов, которые уже загружаются.
        lock (asyncio.Lock): Блокировка для синхронизации доступа к общим ресурсам.
        delay (int): Задержка в секундах между запросами.
        new_urls (list[str]): Список новых URL-адресов, найденных на скачанных страницах.
        group_size (int): Размер группы URL-адресов для параллельной загрузки.
        timeout (int): Время ожидания ответа от сервера в секундах.
        proxy (Optional[str]): URL-адрес прокси-сервера.

    Yields:
        str: Имя скачанного файла.
    """
```

**Назначение**:
Асинхронная функция `download_urls` скачивает файлы