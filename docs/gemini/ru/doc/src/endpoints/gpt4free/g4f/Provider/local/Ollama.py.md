# Модуль для работы с провайдером Ollama
=================================================

Модуль содержит класс :class:`Ollama`, который используется для взаимодействия с локально развернутой моделью Ollama. Он наследуется от класса `OpenaiAPI` и предоставляет методы для получения списка моделей и создания асинхронного генератора.

Пример использования
----------------------

```python
from src.endpoints.gpt4free.g4f.Provider.local.Ollama import Ollama

models = Ollama.get_models()
print(models)
```

## Оглавление

- [Обзор](#обзор)
- [Подробнее](#подробнее)
- [Классы](#классы)
    - [Ollama](#Ollama)
        - [Атрибуты](#атрибуты)
        - [Методы](#методы)
            - [get_models](#get_models)
            - [create_async_generator](#create_async_generator)

## Обзор

Модуль предоставляет класс `Ollama`, который является провайдером для работы с локально развернутой моделью Ollama. Он позволяет получать список доступных моделей и создавать асинхронные генераторы для взаимодействия с моделью.

## Подробнее

Модуль предназначен для интеграции с локально развернутой моделью Ollama. Он использует переменные окружения `OLLAMA_HOST` и `OLLAMA_PORT` для определения хоста и порта, на котором запущена модель. Если переменные окружения не установлены, используются значения по умолчанию `127.0.0.1` и `11434` соответственно.

## Классы

### `Ollama`

**Описание**: Класс для взаимодействия с локально развернутой моделью Ollama.

**Наследует**:
- `OpenaiAPI`: Наследует функциональность для работы с API OpenAI.

#### Атрибуты:

- `label` (str): Метка провайдера, значение "Ollama".
- `url` (str): URL для Ollama, значение "https://ollama.com".
- `login_url` (Optional[str]): URL для логина, значение `None`.
- `needs_auth` (bool): Флаг, указывающий на необходимость аутентификации, значение `False`.
- `working` (bool): Флаг, указывающий на работоспособность провайдера, значение `True`.

#### Методы:

- `get_models`
- `create_async_generator`

### `get_models`

```python
    @classmethod
    def get_models(cls, api_base: str = None, **kwargs):
        """Получает список доступных моделей Ollama.

        Args:
            api_base (str, optional): Базовый URL API. По умолчанию `None`.
            **kwargs: Дополнительные аргументы.

        Returns:
            list[str]: Список названий моделей.

        Raises:
            requests.exceptions.RequestException: Если возникает ошибка при выполнении запроса к API.
        """
        ...
```

**Назначение**: Получает список доступных моделей Ollama.

**Параметры**:

- `api_base` (str, optional): Базовый URL API. По умолчанию `None`. Если `None`, то URL формируется на основе переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`.
- `**kwargs`: Дополнительные аргументы.

**Возвращает**:

- `list[str]`: Список названий моделей.

**Вызывает исключения**:

- `requests.exceptions.RequestException`: Если возникает ошибка при выполнении запроса к API.

**Как работает функция**:

1. **Проверка наличия моделей в кэше:** Функция проверяет, сохранен ли уже список моделей в атрибуте класса `cls.models`. Если список уже есть, он возвращается из кэша.
2. **Определение URL API:** Если `api_base` не указан, функция пытается получить хост и порт из переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`. Если переменные окружения не установлены, используются значения по умолчанию (`127.0.0.1` и `11434` соответственно). На основе хоста и порта формируется URL для запроса списка моделей. Если `api_base` указан, то из него извлекается хост и порт.
3. **Выполнение запроса к API:** Функция выполняет GET-запрос к API Ollama для получения списка моделей.
4. **Обработка ответа API:** Функция извлекает список моделей из JSON-ответа API и сохраняет его в атрибуте класса `cls.models`.
5. **Установка модели по умолчанию:** Функция устанавливает первую модель из списка в качестве модели по умолчанию (`cls.default_model`).
6. **Возврат списка моделей:** Функция возвращает список названий моделей.

```
    A (Проверка кэша)
    │
    ├── Нет -> B (Определение URL API)
    │       │
    │       ├── api_base указан -> C (Использовать api_base)
    │       │
    │       └── api_base не указан -> D (Получение хоста и порта из переменных окружения)
    │
    └── Да -> E (Вернуть список моделей из кэша)
    │
    C/D
    │
    └── F (Выполнение GET-запроса к API)
    │
    F (Обработка ответа API)
    │
    ├── G (Извлечение списка моделей из JSON)
    │
    └── H (Сохранение списка моделей в кэше)
    │
    I (Установка модели по умолчанию)
    │
    J (Возврат списка моделей)
```

**Примеры**:

```python
from src.endpoints.gpt4free.g4f.Provider.local.Ollama import Ollama

# Пример 1: Получение списка моделей с использованием значений по умолчанию
models = Ollama.get_models()
print(models)

# Пример 2: Получение списка моделей с указанием api_base
models = Ollama.get_models(api_base="http://localhost:11434/v1")
print(models)
```

### `create_async_generator`

```python
    @classmethod
    def create_async_generator(
        cls,
        model: str,
        messages: Messages,
        api_base: str = None,
        **kwargs
    ) -> AsyncResult:
        """Создает асинхронный генератор для взаимодействия с моделью Ollama.

        Args:
            model (str): Название модели.
            messages (Messages): Список сообщений для отправки модели.
            api_base (str, optional): Базовый URL API. По умолчанию `None`.
            **kwargs: Дополнительные аргументы.

        Returns:
            AsyncResult: Асинхронный генератор для получения ответов от модели.
        """
        ...
```

**Назначение**: Создает асинхронный генератор для взаимодействия с моделью Ollama.

**Параметры**:

- `model` (str): Название модели.
- `messages` (Messages): Список сообщений для отправки модели.
- `api_base` (str, optional): Базовый URL API. По умолчанию `None`. Если `None`, то URL формируется на основе переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`.
- `**kwargs`: Дополнительные аргументы.

**Возвращает**:

- `AsyncResult`: Асинхронный генератор для получения ответов от модели.

**Как работает функция**:

1. **Определение URL API:** Если `api_base` не указан, функция пытается получить хост и порт из переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`. Если переменные окружения не установлены, используются значения по умолчанию (`localhost` и `11434` соответственно). На основе хоста и порта формируется URL для запроса к API.
2. **Вызов метода суперкласса:** Функция вызывает метод `create_async_generator` суперкласса (`OpenaiAPI`) с указанием модели, сообщений и базового URL API.

```
    A (Определение URL API)
    │
    ├── api_base указан -> B (Использовать api_base)
    │
    └── api_base не указан -> C (Получение хоста и порта из переменных окружения)
    │
    B/C
    │
    └── D (Вызов метода create_async_generator суперкласса)
    │
    E (Возврат асинхронного генератора)
```

**Примеры**:

```python
from src.endpoints.gpt4free.g4f.Provider.local.Ollama import Ollama

# Пример: Создание асинхронного генератора для взаимодействия с моделью Ollama
model = "llama2"
messages = [{"role": "user", "content": "Hello, Ollama!"}]
generator = Ollama.create_async_generator(model=model, messages=messages)