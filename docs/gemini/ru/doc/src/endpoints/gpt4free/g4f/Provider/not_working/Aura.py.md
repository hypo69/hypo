# Модуль `Aura`

## Обзор

Модуль `Aura` представляет собой асинхронный провайдер для взаимодействия с моделью OpenChat 3.6 через API сервиса openchat.team. Он позволяет отправлять запросы к модели и получать ответы в асинхронном режиме, поддерживая прокси и другие параметры конфигурации. Модуль использует `aiohttp` для выполнения асинхронных HTTP-запросов.

## Подробнее

Модуль предназначен для интеграции с другими частями проекта `hypotez`, где требуется взаимодействие с AI-моделями через API. `Aura` предоставляет асинхронный генератор, который позволяет получать ответы от модели частями, что полезно для обработки больших объемов данных.

## Функции

### `create_async_generator`

```python
@classmethod
async def create_async_generator(
    cls,
    model: str,
    messages: Messages,
    proxy: str = None,
    temperature: float = 0.5,
    max_tokens: int = 8192,
    webdriver = None,
    **kwargs
) -> AsyncResult:
    """
    Создает асинхронный генератор для взаимодействия с моделью OpenChat 3.6.

    Args:
        cls (type): Ссылка на класс.
        model (str): Идентификатор модели (не используется, но должен быть передан).
        messages (Messages): Список сообщений для отправки в модель.
        proxy (str, optional): Адрес прокси-сервера для использования. По умолчанию `None`.
        temperature (float, optional): Температура модели. По умолчанию 0.5.
        max_tokens (int, optional): Максимальное количество токенов в ответе. По умолчанию 8192.
        webdriver: Драйвер браузера для получения аргументов. По умолчанию `None`.
        **kwargs: Дополнительные параметры.

    Returns:
        AsyncResult: Асинхронный генератор, выдающий части ответа модели.

    Raises:
        aiohttp.ClientResponseError: Если HTTP-запрос завершается с ошибкой.

    """
```

**Назначение**: Создает и возвращает асинхронный генератор, который отправляет сообщения в модель OpenChat 3.6 и возвращает результаты частями.

**Параметры**:
- `cls` (type): Ссылка на класс.
- `model` (str): Идентификатор модели.
- `messages` (Messages): Список сообщений для отправки в модель.
- `proxy` (str, optional): Адрес прокси-сервера для использования. По умолчанию `None`.
- `temperature` (float, optional): Температура модели. По умолчанию 0.5.
- `max_tokens` (int, optional): Максимальное количество токенов в ответе. По умолчанию 8192.
- `webdriver`: Драйвер браузера для получения аргументов. По умолчанию `None`.
- `**kwargs`: Дополнительные параметры.

**Возвращает**:
- `AsyncResult`: Асинхронный генератор, выдающий части ответа модели.

**Вызывает исключения**:
- `aiohttp.ClientResponseError`: Если HTTP-запрос завершается с ошибкой.

**Как работает функция**:

1. **Извлечение аргументов из браузера**:
   - Функция `get_args_from_browser` вызывается для получения аргументов сессии из браузера, используя переданный `webdriver` и `proxy`.

2. **Создание асинхронной сессии**:
   - Используется `aiohttp.ClientSession` для создания асинхронной сессии с полученными аргументами.

3. **Обработка сообщений**:
   - Разделяет входящие сообщения на системные и обычные сообщения. Системные сообщения объединяются в строку для использования в запросе.

4. **Формирование данных запроса**:
   - Формирует словарь `data`, который включает идентификатор модели, список сообщений, ключ API (пустой), системное сообщение и параметры температуры и максимального количества токенов.

5. **Отправка POST-запроса**:
   - Отправляет асинхронный POST-запрос к API сервиса (`f"{cls.url}/api/chat"`) с данными в формате JSON и прокси-сервером (если указан).

6. **Обработка ответа**:
   - Для каждого чанка в ответе выполняется декодирование (`chunk.decode(error="ignore")`), и результат передается через `yield`, делая функцию генератором.

```text
Начало
   ↓
Получение аргументов из браузера (get_args_from_browser)
   ↓
Создание асинхронной сессии (ClientSession)
   ↓
Разделение сообщений на системные и обычные
   ↓
Формирование данных запроса (data)
   ↓
Отправка POST-запроса к API (/api/chat)
   ↓
Обработка и декодирование чанков ответа
   ↓
Выдача чанков через yield
   ↓
Конец
```

**Примеры**:

```python
# Пример использования асинхронного генератора
import asyncio
from typing import List, Dict

from src.endpoints.gpt4free.g4f.Provider.not_working.Aura import Aura

async def main():
    messages: List[Dict[str, str]] = [
        {"role": "user", "content": "Hello, how are you?"}
    ]
    async for chunk in Aura.create_async_generator(model="openchat_3.6", messages=messages):
        print(chunk, end="")

if __name__ == "__main__":
    asyncio.run(main())