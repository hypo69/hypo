# Модуль `AutonomousAI`

## Обзор

Модуль `AutonomousAI` предоставляет асинхронный генератор для взаимодействия с различными моделями AI, размещенными на платформе `autonomous.ai`. Он поддерживает потоковую передачу данных и системные сообщения, обеспечивая гибкость при работе с AI-агентами.

## Подробней

Модуль предназначен для асинхронного взаимодействия с API `autonomous.ai` для получения ответов от различных AI-моделей, таких как `llama`, `qwen_coder`, `hermes`, `vision` и `summary`. Он использует `aiohttp` для выполнения асинхронных HTTP-запросов и `base64` для кодирования сообщений. Поддерживает потоковую передачу данных, что позволяет получать ответы в режиме реального времени.

## Классы

### `AutonomousAI`

**Описание**: Класс `AutonomousAI` является асинхронным генератором и предоставляет интерфейс для взаимодействия с AI-моделями на платформе `autonomous.ai`.
**Наследует**:
- `AsyncGeneratorProvider`: Обеспечивает асинхронную генерацию данных.
- `ProviderModelMixin`: Предоставляет функциональность для работы с моделями.

**Атрибуты**:

- `url` (str): Базовый URL для `autonomous.ai`.
- `api_endpoints` (dict): Словарь, содержащий URL-адреса API для различных моделей.
- `working` (bool): Указывает, работает ли провайдер.
- `supports_stream` (bool): Указывает, поддерживает ли провайдер потоковую передачу данных.
- `supports_system_message` (bool): Указывает, поддерживает ли провайдер системные сообщения.
- `supports_message_history` (bool): Указывает, поддерживает ли провайдер историю сообщений.
- `default_model` (str): Модель, используемая по умолчанию (`llama`).
- `models` (list): Список поддерживаемых моделей.
- `model_aliases` (dict): Словарь псевдонимов моделей.

**Методы**:

- `create_async_generator()`: Создает асинхронный генератор для получения ответов от AI-модели.

## Функции

### `create_async_generator`

```python
@classmethod
async def create_async_generator(
    cls,
    model: str,
    messages: Messages,
    proxy: str = None,
    stream: bool = False,
    **kwargs
) -> AsyncResult:
    """Создает асинхронный генератор для получения ответов от AI-модели.

    Args:
        model (str): Название модели для использования.
        messages (Messages): Список сообщений для отправки в AI-модель.
        proxy (str, optional): URL прокси-сервера для использования. По умолчанию `None`.
        stream (bool, optional): Включает или выключает потоковую передачу данных. По умолчанию `False`.
        **kwargs: Дополнительные аргументы.

    Returns:
        AsyncResult: Асинхронный генератор, возвращающий ответы от AI-модели.

    Raises:
        Exception: В случае ошибки при выполнении запроса.

    """
```

**Назначение**: Создает асинхронный генератор, который взаимодействует с API `autonomous.ai` для получения ответов от указанной AI-модели.

**Параметры**:

- `cls`: Ссылка на класс `AutonomousAI`.
- `model` (str): Имя модели, которую нужно использовать (например, "llama", "qwen_coder").
- `messages` (Messages): Список сообщений, которые будут отправлены AI-модели. Сообщения должны быть в формате, ожидаемом API `autonomous.ai`.
- `proxy` (str, optional): URL прокси-сервера, если необходимо использовать прокси для подключения к API. По умолчанию `None`.
- `stream` (bool, optional): Флаг, указывающий, следует ли использовать потоковую передачу данных. Если `True`, ответы будут возвращаться по частям по мере их генерации. По умолчанию `False`.
- `**kwargs`: Дополнительные параметры, которые могут быть переданы в API.

**Возвращает**:

- `AsyncResult`: Асинхронный генератор, который выдает текстовые фрагменты ответа от AI-модели.

**Вызывает исключения**:

- `aiohttp.ClientError`: Если возникает ошибка при выполнении HTTP-запроса.
- `json.JSONDecodeError`: Если не удается декодировать JSON-ответ от API.

**Как работает функция**:

1. **Определение endpoint'а**: На основе переданной модели (`model`) выбирается соответствующий endpoint API из словаря `cls.api_endpoints`.
2. **Формирование заголовков**: Создаются заголовки HTTP-запроса, включая `Content-Type`, `Origin`, `Referer` и `User-Agent`.
3. **Создание сессии**: Используется `aiohttp.ClientSession` для выполнения асинхронного HTTP-запроса.
4. **Кодирование сообщений**: Список сообщений (`messages`) преобразуется в JSON-строку, которая затем кодируется в Base64.
5. **Формирование данных запроса**: Подготавливаются данные для отправки в теле запроса, включая закодированное сообщение, идентификатор потока (`threadId`), флаг потоковой передачи (`stream`) и идентификатор AI-агента (`aiAgent`).
6. **Выполнение POST-запроса**: Отправляется POST-запрос к API endpoint с использованием `session.post()`. Если указан прокси-сервер, он также используется.
7. **Обработка ответа**: Полученный ответ обрабатывается по частям (chunks).
8. **Декодирование чанков**: Каждый чанк декодируется и преобразуется в строку.
9. **Обработка данных**: Если чанк содержит данные (`data: `), он очищается от префикса `data: ` и декодируется как JSON. Извлеченные данные передаются в генератор.
10. **Обработка ошибок**: В случае возникновения ошибки декодирования JSON, она игнорируется.
11. **Завершение потока**: Когда приходит сообщение `data: [DONE]`, генератор завершает свою работу.

**Внутренние функции**: Нет

**ASCII flowchart**:

```
  Начало
   ↓
Выбор API Endpoint
   ↓
Создание HTTP заголовков
   ↓
  Создание Aiohttp сессии
   ↓
JSON кодирование сообщений
   ↓
Base64 кодирование сообщений
   ↓
Подготовка данных для POST запроса
   ↓
  Выполнение POST запроса к API
   ↓
  Обработка ответа (по частям)
   ↓
Декодирование JSON чанков
   ↓
Извлечение данных из JSON
   ↓
Передача данных в генератор
   ↓
   Конец (data: [DONE])
```

**Примеры**:

```python
# Пример 1: Использование с потоковой передачей данных
messages = [{"role": "user", "content": "Tell me a joke."}]
async for message in AutonomousAI.create_async_generator(model="llama", messages=messages, stream=True):
    print(message, end="")

# Пример 2: Использование без потоковой передачи данных
messages = [{"role": "user", "content": "Summarize the article."}]
async for message in AutonomousAI.create_async_generator(model="summary", messages=messages):
    print(message, end="")

# Пример 3: Использование с прокси-сервером
messages = [{"role": "user", "content": "What is the weather in New York?"}]
async for message in AutonomousAI.create_async_generator(model="llama", messages=messages, proxy="http://your_proxy:8080"):
    print(message, end="")