# Модуль `web_search`

## Обзор

Модуль предоставляет инструменты для выполнения веб-поиска с использованием DuckDuckGo Search и извлечения текста с веб-страниц. Он включает функции для поиска, извлечения текста и форматирования результатов поиска.

## Подробнее

Этот модуль используется для получения информации из интернета на основе поискового запроса. Он интегрируется с библиотекой `duckduckgo-search` для выполнения поисковых запросов и `beautifulsoup4` для извлечения текста из HTML-контента. Модуль также включает кэширование результатов поиска для повышения производительности.

## Содержание

- [Классы](#Классы)
    - [SearchResults](#SearchResults)
    - [SearchResultEntry](#SearchResultEntry)
- [Функции](#Функции)
    - [scrape_text](#scrape_text)
    - [fetch_and_scrape](#fetch_and_scrape)
    - [search](#search)
    - [do_search](#do_search)
    - [get_search_message](#get_search_message)
    - [spacy_get_keywords](#spacy_get_keywords)

## Классы

### `SearchResults`

**Описание**: Класс представляет результаты веб-поиска.

**Аттрибуты**:
- `results` (list): Список объектов `SearchResultEntry`, представляющих отдельные результаты поиска.
- `used_words` (int): Количество использованных слов в результатах поиска.

**Методы**:
- `from_dict(data: dict)`: Создает объект `SearchResults` из словаря.
- `__iter__()`: Итератор по результатам поиска.
- `__str__()`: Возвращает строковое представление результатов поиска.
- `__len__()` -> int: Возвращает количество результатов поиска.
- `get_sources() -> Sources`: Возвращает объект `Sources`, содержащий URL и заголовки результатов поиска.
- `get_dict()`: Возвращает словарь, представляющий объект `SearchResults`.

### `SearchResultEntry`

**Описание**: Класс представляет отдельный результат веб-поиска.

**Аттрибуты**:
- `title` (str): Заголовок результата поиска.
- `url` (str): URL результата поиска.
- `snippet` (str): Краткое описание результата поиска.
- `text` (str, optional): Полный текст, извлеченный из результата поиска. По умолчанию `None`.

**Методы**:
- `set_text(text: str)`: Устанавливает полный текст для результата поиска.
- `get_dict()`:  Возвращает словарь, представляющий объект `SearchResultEntry`.

## Функции

### `scrape_text`

```python
def scrape_text(html: str, max_words: int = None, add_source=True, count_images: int = 2) -> Iterator[str]:
    """
    Извлекает текст из HTML-контента, удаляя лишние элементы и форматируя результат.

    Args:
        html (str): HTML-контент для извлечения текста.
        max_words (int, optional): Максимальное количество слов для извлечения. По умолчанию `None` (без ограничений).
        add_source (bool, optional): Добавлять ли источник в конце извлеченного текста. По умолчанию `True`.
        count_images (int, optional): Количество изображений, которые нужно добавить в результат. По умолчанию 2.

    Returns:
        Iterator[str]: Итератор строк, содержащих извлеченный текст.

    Как работает функция:
    1.  Преобразует HTML в объект `BeautifulSoup` для удобной навигации и извлечения данных.
    2.  Выбирает основной контент страницы, пытаясь найти его в различных распространенных селекторах (таких как "main", ".main-content-wrapper" и т.д.). Это позволяет выделить наиболее важную часть страницы для извлечения текста.
    3.  Удаляет ненужные элементы, такие как глобальные элементы раскрытия информации (например, ".c-globalDisclosure" на Zdnet), чтобы избежать включения нерелевантного контента.
    4.  Извлекает текст из различных элементов, таких как заголовки (h1-h6), параграфы (p), предварительно отформатированный текст (pre), таблицы (table) и списки (ul), а также ссылки на изображения.
    5.  Форматирует изображения, добавляя их в результат с использованием специального синтаксиса `!\[alt-text](image_url)`, где alt-text берется из атрибута "title" элемента `paragraph` или его текстового содержимого.
    6.  Разделяет текст на строки и слова, фильтруя пустые строки и дубликаты, чтобы получить чистый и уникальный текстовый контент.
    7.  Ограничивает количество извлекаемых слов, если указан параметр `max_words`, чтобы избежать извлечения слишком большого объема текста.
    8.  Добавляет информацию об источнике, если `add_source` установлен в `True`, извлекая каноническую ссылку из HTML и форматируя ее в виде `Source: [domain](link)`.

    ASCII схема работы функции:

    ```
    HTML --> BeautifulSoup --> Выбор контента --> Удаление элементов --> Извлечение текста --> Форматирование изображений -->
    Разделение на строки и слова --> Фильтрация --> Ограничение по количеству слов --> Добавление источника --> Итератор строк
    ```

    Примеры:
    ```python
    from bs4 import BeautifulSoup
    html = "<html><body><h1>Заголовок</h1><p>Текст параграфа</p></body></html>"
    result = list(scrape_text(html))
    print(result)  # ['Заголовок\n', 'Текст параграфа\n', '\nSource: [None](None)']
    ```
    """
    ...
```

### `fetch_and_scrape`

```python
async def fetch_and_scrape(session: ClientSession, url: str, max_words: int = None, add_source: bool = False) -> str:
    """
    Асинхронно извлекает HTML-контент из URL и извлекает текст с использованием `scrape_text`. Кэширует результаты.

    Args:
        session (ClientSession): Асинхронная клиентская сессия aiohttp.
        url (str): URL для извлечения контента.
        max_words (int, optional): Максимальное количество слов для извлечения. По умолчанию `None` (без ограничений).
        add_source (bool, optional): Добавлять ли источник в конце извлеченного текста. По умолчанию `False`.

    Returns:
        str: Извлеченный текст или `None` в случае ошибки.

    Как работает функция:
    1.  Определяет путь к директории кэша и создает её, если она не существует. Директория кэша используется для хранения результатов извлечения текста, чтобы избежать повторных запросов к одним и тем же URL.
    2.  Вычисляет MD5-хеш URL для создания уникального имени файла кэша. Это позволяет избежать конфликтов имен файлов и обеспечивает быстрый поиск кэшированных результатов.
    3.  Проверяет, существует ли кэшированный файл для данного URL. Если файл существует, функция считывает текст из файла и возвращает его, не выполняя повторный запрос к URL.
    4.  Выполняет асинхронный GET-запрос к URL с использованием предоставленной сессии `aiohttp.ClientSession`.
    5.  Если запрос успешен (код состояния 200), функция извлекает HTML-контент из ответа и использует функцию `scrape_text` для извлечения текста из HTML.
    6.  Сохраняет извлеченный текст в кэшированный файл для последующего использования.
    7.  Возвращает извлеченный текст.
    8.  Обрабатывает исключения `ClientError` и `asyncio.TimeoutError`, возвращая `None` в случае ошибки.

    ASCII схема работы функции:

    ```
    URL --> MD5-хеш --> Проверка кэша --> Чтение из кэша / GET-запрос --> Извлечение текста (scrape_text) --> Сохранение в кэш --> Возврат текста
    ```

    Примеры:
    ```python
    import asyncio
    from aiohttp import ClientSession

    async def main():
        async with ClientSession() as session:
            url = "https://www.example.com"
            text = await fetch_and_scrape(session, url, max_words=100, add_source=True)
            print(text)

    if __name__ == "__main__":
        asyncio.run(main())
    ```
    """
    ...
```

### `search`

```python
async def search(query: str, max_results: int = 5, max_words: int = 2500, backend: str = "auto", add_text: bool = True, timeout: int = 5, region: str = "wt-wt") -> SearchResults:
    """
    Выполняет поиск с использованием DuckDuckGo Search и возвращает результаты.

    Args:
        query (str): Поисковый запрос.
        max_results (int, optional): Максимальное количество результатов для возврата. По умолчанию 5.
        max_words (int, optional): Максимальное количество слов для извлечения из каждого результата. По умолчанию 2500.
        backend (str, optional): Бэкэнд для использования DuckDuckGo Search. По умолчанию "auto".
        add_text (bool, optional): Извлекать ли полный текст из результатов поиска. По умолчанию `True`.
        timeout (int, optional): Таймаут для HTTP-запросов. По умолчанию 5.
        region (str, optional): Регион для поиска. По умолчанию "wt-wt".

    Returns:
        SearchResults: Объект `SearchResults`, содержащий результаты поиска.

    Raises:
        MissingRequirementsError: Если не установлены необходимые зависимости (`duckduckgo-search` и `beautifulsoup4`).

    Как работает функция:
    1.  Проверяет, установлены ли необходимые зависимости (`duckduckgo-search` и `beautifulsoup4`). Если нет, вызывает исключение `MissingRequirementsError`.
    2.  Выполняет поиск с использованием `ddgs.text`, передавая поисковый запрос, регион, параметры безопасного поиска, ограничение по времени и максимальное количество результатов.
    3.  Фильтрует результаты, исключая результаты, содержащие ".google.".
    4.  Создает список объектов `SearchResultEntry` из результатов поиска.
    5.  Если `add_text` установлен в `True`, выполняет асинхронные запросы к URL каждого результата поиска для извлечения полного текста с использованием `fetch_and_scrape`.
    6.  Форматирует результаты, ограничивая общее количество слов в результатах поиска в соответствии с параметром `max_words`.
    7.  Возвращает объект `SearchResults`, содержащий отформатированные результаты поиска и количество использованных слов.

    ASCII схема работы функции:

    ```
    Запрос --> Проверка зависимостей --> DuckDuckGo Search --> Фильтрация --> Создание SearchResultEntry -->
    Асинхронное извлечение текста (fetch_and_scrape) --> Форматирование результатов --> Возврат SearchResults
    ```

    Примеры:
    ```python
    import asyncio

    async def main():
        results = await search("Python programming", max_results=3, max_words=1000)
        print(results)

    if __name__ == "__main__":
        asyncio.run(main())
    ```
    """
    ...
```

### `do_search`

```python
async def do_search(prompt: str, query: str = None, instructions: str = DEFAULT_INSTRUCTIONS, **kwargs) -> tuple[str, Sources]:
    """
    Выполняет поиск и форматирует результаты для использования в запросах к языковой модели.

    Args:
        prompt (str): Исходный запрос пользователя.
        query (str, optional): Поисковый запрос. Если `None`, используется первая строка запроса пользователя.
        instructions (str, optional): Инструкции для форматирования результатов поиска. По умолчанию `DEFAULT_INSTRUCTIONS`.
        **kwargs: Дополнительные аргументы для функции `search`.

    Returns:
        tuple[str, Sources]: Кортеж, содержащий отформатированный запрос и объект `Sources` с URL и заголовками результатов поиска.

    Как работает функция:
    1.  Проверяет, нужно ли выполнять поиск. Если инструкции уже добавлены в запрос или запрос начинается с "##" и нет поискового запроса, функция возвращает исходный запрос.
    2.  Определяет поисковый запрос. Если `query` не указан, используется первая строка запроса пользователя.
    3.  Кэширует результаты поиска. Функция создает MD5-хеш JSON-представления параметров запроса и использует его для создания имени файла кэша.
    4.  Проверяет, существует ли кэшированный файл для данного запроса. Если файл существует, функция считывает результаты поиска из файла.
    5.  Если кэшированный файл не существует, функция выполняет поиск с использованием функции `search`.
    6.  Форматирует результаты поиска. Если указаны инструкции, функция добавляет результаты поиска, инструкции и исходный запрос в новый запрос.
    7.  Возвращает отформатированный запрос и объект `Sources` с URL и заголовками результатов поиска.

    ASCII схема работы функции:

    ```
    Запрос --> Проверка необходимости поиска --> Определение поискового запроса --> MD5-хеш параметров -->
    Проверка кэша --> Чтение из кэша / Выполнение поиска (search) --> Форматирование результатов --> Возврат запроса и Sources
    ```

    Примеры:
    ```python
    import asyncio

    async def main():
        prompt = "What is the capital of France?"
        new_prompt, sources = await do_search(prompt)
        print(new_prompt)
        print(sources)

    if __name__ == "__main__":
        asyncio.run(main())
    ```
    """
    ...
```

### `get_search_message`

```python
def get_search_message(prompt: str, raise_search_exceptions=False, **kwargs) -> str:
    """
    Выполняет поиск и возвращает отформатированный запрос.

    Args:
        prompt (str): Исходный запрос пользователя.
        raise_search_exceptions (bool, optional): Вызывать ли исключения, возникающие при поиске. По умолчанию `False`.
        **kwargs: Дополнительные аргументы для функции `do_search`.

    Returns:
        str: Отформатированный запрос.

    Как работает функция:
    1.  Вызывает функцию `do_search` асинхронно с переданными аргументами.
    2.  Перехватывает исключения `DuckDuckGoSearchException` и `MissingRequirementsError`, которые могут возникнуть при поиске.
    3.  Если `raise_search_exceptions` установлен в `True`, исключения перебрасываются.
    4.  Если `raise_search_exceptions` установлен в `False`, исключения логируются, и функция возвращает исходный запрос.
    5.  В случае успешного выполнения возвращает отформатированный запрос.

    ASCII схема работы функции:

    ```
    Запрос --> Выполнение do_search --> Перехват исключений --> Возврат запроса / Переброс исключения
    ```

    Примеры:
    ```python
    prompt = "What is the capital of France?"
    message = get_search_message(prompt)
    print(message)
    ```
    """
    ...
```

### `spacy_get_keywords`

```python
def spacy_get_keywords(text: str):
    """
    Извлекает ключевые слова из текста с использованием библиотеки spaCy.

    Args:
        text (str): Текст для извлечения ключевых слов.

    Returns:
        keywords(List[str]): Список ключевых слов.

    Как работает функция:
    1.  Проверяет, установлена ли библиотека spaCy. Если нет, возвращает исходный текст.
    2.  Загружает языковую модель spaCy ("en_core_web_sm").
    3.  Обрабатывает текст с использованием модели spaCy.
    4.  Извлекает ключевые слова на основе частей речи (существительные, прилагательные) и именованных сущностей.
    5.  Удаляет дубликаты и возвращает список ключевых слов.
    6.  Извлекает именные группы из текста

    ASCII схема работы функции:

    ```
    Текст --> Проверка spaCy --> Загрузка модели --> Обработка текста --> Извлечение ключевых слов --> Удаление дубликатов --> Возврат ключевых слов
    ```

    Примеры:
    ```python
    text = "The quick brown fox jumps over the lazy dog."
    keywords = spacy_get_keywords(text)
    print(keywords)
    ```
    """
    ...
```