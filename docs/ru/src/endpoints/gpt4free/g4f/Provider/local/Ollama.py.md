# Модуль `Ollama.py`

## Обзор

Модуль `Ollama.py` предоставляет класс `Ollama`, который является адаптером для взаимодействия с локально установленным сервером Ollama. Ollama позволяет запускать большие языковые модели локально. Модуль наследует функциональность от класса `OpenaiAPI` и предоставляет методы для получения списка доступных моделей и создания асинхронного генератора для взаимодействия с моделью.

## Подробнее

Этот модуль предназначен для интеграции с сервером Ollama, позволяя использовать локально размещенные модели вместо облачных сервисов. Это может быть полезно для задач, требующих конфиденциальности или работы в условиях ограниченного доступа к интернету.  Он позволяет опрашивать сервер Ollama для получения списка доступных моделей и использовать эти модели для генерации текста.

## Классы

### `Ollama`

**Описание**: Класс `Ollama` предоставляет интерфейс для взаимодействия с сервером Ollama.

**Наследует**:
- `OpenaiAPI`: Наследует методы для работы с API, такие как создание асинхронных генераторов.

**Атрибуты**:
- `label` (str): Метка, идентифицирующая провайдера как "Ollama".
- `url` (str): URL официального сайта Ollama.
- `login_url` (Optional[str]): URL для входа в систему (в данном случае `None`, так как аутентификация не требуется).
- `needs_auth` (bool): Флаг, указывающий, требуется ли аутентификация (в данном случае `False`).
- `working` (bool): Флаг, указывающий, работает ли провайдер (в данном случае `True`).
- `models` (List[str]): Список доступных моделей, полученный от сервера Ollama.
- `default_model` (str): Модель по умолчанию.

**Методы**:
- `get_models()`: Получает список доступных моделей с сервера Ollama.
- `create_async_generator()`: Создает асинхронный генератор для взаимодействия с моделью Ollama.

## Функции

### `get_models`

```python
    @classmethod
    def get_models(cls, api_base: str = None, **kwargs):
        """
        Получает список доступных моделей с сервера Ollama.

        Args:
            api_base (str, optional): Базовый URL API. По умолчанию `None`.
            **kwargs: Дополнительные параметры.

        Returns:
            List[str]: Список доступных моделей.
        """
        ...
```

**Назначение**: Получение списка моделей, доступных на сервере Ollama.

**Параметры**:
- `api_base` (str, optional): Базовый URL API. Если не указан, используется значение из переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`. По умолчанию `None`.
- `**kwargs`: Дополнительные параметры, которые могут быть переданы в запросе.

**Возвращает**:
- `List[str]`: Список доступных моделей.

**Как работает функция**:

1. **Проверка кэша**: Функция сначала проверяет, был ли уже получен список моделей и сохранен в атрибуте `cls.models`. Если список уже есть, он возвращается из кэша.
2. **Определение URL**: Если список моделей отсутствует в кэше, функция определяет URL для запроса к API Ollama. Если `api_base` не указан, URL формируется на основе переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`. В противном случае, `api_base` используется для формирования URL.
3. **Запрос к API**: Функция отправляет GET-запрос к API Ollama по определенному URL для получения списка моделей.
4. **Обработка ответа**: Полученный JSON-ответ преобразуется в список имен моделей, который сохраняется в атрибуте `cls.models` и возвращается.

```
A: Проверка кэша (cls.models)
|
B: Определение URL API (из api_base или переменных окружения)
|
C: GET-запрос к API Ollama
|
D: Обработка JSON-ответа и извлечение имен моделей
|
E: Сохранение списка моделей в кэше (cls.models)
|
F: Возврат списка моделей
```

**Примеры**:

```python
# Пример вызова без указания api_base (используются переменные окружения)
models = Ollama.get_models()
print(models)

# Пример вызова с указанием api_base
models = Ollama.get_models(api_base="http://localhost:11434/v1")
print(models)
```

### `create_async_generator`

```python
    @classmethod
    def create_async_generator(
        cls,
        model: str,
        messages: Messages,
        api_base: str = None,
        **kwargs
    ) -> AsyncResult:
        """
        Создает асинхронный генератор для взаимодействия с моделью Ollama.

        Args:
            model (str): Имя модели для использования.
            messages (Messages): Список сообщений для отправки модели.
            api_base (str, optional): Базовый URL API. По умолчанию `None`.
            **kwargs: Дополнительные параметры.

        Returns:
            AsyncResult: Асинхронный генератор для получения ответов от модели.
        """
        ...
```

**Назначение**: Создание асинхронного генератора для взаимодействия с моделью Ollama.

**Параметры**:
- `model` (str): Имя модели для использования.
- `messages` (Messages): Список сообщений для отправки модели.
- `api_base` (str, optional): Базовый URL API. Если не указан, используется значение из переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`. По умолчанию `None`.
- `**kwargs`: Дополнительные параметры, которые могут быть переданы в запросе.

**Возвращает**:
- `AsyncResult`: Асинхронный генератор для получения ответов от модели.

**Как работает функция**:

1. **Определение URL**: Функция определяет URL API Ollama. Если `api_base` не указан, URL формируется на основе переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`.
2. **Вызов родительского метода**: Функция вызывает метод `create_async_generator` родительского класса `OpenaiAPI` с переданными параметрами и определенным `api_base`.

```
A: Определение URL API (из api_base или переменных окружения)
|
B: Вызов create_async_generator родительского класса (OpenaiAPI)
|
C: Возврат асинхронного генератора
```

**Примеры**:

```python
# Пример вызова с указанием модели и сообщений
messages = [{"role": "user", "content": "Hello, Ollama!"}]
generator = Ollama.create_async_generator(model="llama2", messages=messages)

# Пример вызова с указанием api_base
generator = Ollama.create_async_generator(model="llama2", messages=messages, api_base="http://localhost:11434/v1")