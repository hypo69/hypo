Как использовать этот блок кода
=========================================================================================

Описание
-------------------------
Этот код реализует веб-скрабер для извлечения данных с сайта.  Он использует библиотеку Playwright для управления браузером, извлечения данных и обработки ссылок. Код организован в классе `CrawleePython`, предоставляющем методы для настройки, запуска и экспорта данных, полученных с веб-сайтов.

Шаги выполнения
-------------------------
1. **Инициализация `CrawleePython`**: Создается экземпляр класса `CrawleePython` с параметрами:
    - `max_requests`: Максимальное количество запросов, которые будет обрабатывать скрабер.
    - `headless`: Флаг, определяющий, будет ли браузер работать в фоновом режиме (без отображения окна).
    - `browser_type`: Тип браузера (например, 'firefox', 'chromium').
2. **Настройка скрабера (`setup_crawler`)**: Этот метод инициализирует объект `PlaywrightCrawler`, определяя максимальное количество одновременных запросов, режим работы браузера и тип браузера.  Также задаётся обработчик `request_handler` для каждой полученной ссылки.
3. **Обработка запроса (`request_handler`)**:  В этом обработчике выполняются следующие действия:
    - Запись сообщения о текущем обрабатываемом URL.
    - Добавление найденных ссылок в очередь для дальнейшей обработки.
    - Извлечение данных с текущей страницы (URL, заголовок, содержимое). Содержимое обрезается до первых 100 символов.
    - Отправка извлечённых данных в хранилище.
4. **Запуск скрабера (`run_crawler`)**: Метод запускает скрабинг для указанного списка URL.
5. **Экспорт данных (`export_data`)**:  Сохраняет извлечённые данные в JSON-файл по заданному пути.
6. **Получение данных (`get_data`)**: Возвращает извлечённые данные в виде словаря.
7. **Основной метод (`run`)**:  Объединяет все предыдущие этапы.  Вызывает `setup_crawler`, `run_crawler`, `export_data` и `get_data`, обрабатывает ошибки и выводит информацию о полученных данных.


Пример использования
-------------------------
.. code-block:: python

    import asyncio
    from src import gs
    from crawlee_python import CrawleePython

    async def main():
        experiment = CrawleePython(max_requests=5, headless=False, browser_type='firefox')
        urls = ['https://ksp.co.il']
        await experiment.run(urls)

    asyncio.run(main())