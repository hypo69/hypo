# Модуль для работы с AiAsk
==========================

Модуль предоставляет класс `AiAsk`, который используется для взаимодействия с сервисом AiAsk.me.
Этот модуль предназначен для асинхронного взаимодействия с API AiAsk, поддерживающего ведение истории сообщений и модель GPT-3.5 Turbo.

## Обзор

Модуль содержит класс `AiAsk`, который является подклассом `AsyncGeneratorProvider`. Он позволяет отправлять запросы к API AiAsk и получать ответы в асинхронном режиме.

## Подробнее

Модуль предназначен для использования в асинхронных приложениях, где требуется взаимодействие с API AiAsk. Он обеспечивает поддержку прокси и позволяет настраивать параметры запроса, такие как температура.

## Классы

### `AiAsk`

**Описание**:
Класс `AiAsk` предоставляет функциональность для взаимодействия с API AiAsk.me.

**Наследует**:
- `AsyncGeneratorProvider`: класс, предоставляющий базовую функциональность для асинхронных провайдеров, генерирующих данные.

**Атрибуты**:
- `url` (str): URL сервиса AiAsk.me.
- `supports_message_history` (bool): Флаг, указывающий на поддержку истории сообщений.
- `supports_gpt_35_turbo` (bool): Флаг, указывающий на поддержку модели GPT-3.5 Turbo.
- `working` (bool): Флаг, указывающий на работоспособность провайдера.

### `create_async_generator`

```python
 @classmethod
    async def create_async_generator(
        cls,
        model: str,
        messages: Messages,
        proxy: str = None,
        **kwargs
    ) -> AsyncResult:
        """
        Создает асинхронный генератор для взаимодействия с API AiAsk.

        Args:
            model (str): Модель для использования.
            messages (Messages): Список сообщений для отправки.
            proxy (str, optional): Прокси-сервер для использования. По умолчанию `None`.
            **kwargs: Дополнительные аргументы.

        Returns:
            AsyncResult: Асинхронный генератор, выдающий ответы от API AiAsk.

        Raises:
            RuntimeError: Если достигнут лимит запросов.

        Пример:
            async for chunk in AiAsk.create_async_generator(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello"}], proxy="http://proxy.example.com"):
                print(chunk)
        """
```

**Назначение**:
Создает асинхронный генератор для взаимодействия с API AiAsk.

**Параметры**:
- `cls` (AiAsk): Ссылка на класс `AiAsk`.
- `model` (str): Модель для использования.
- `messages` (Messages): Список сообщений для отправки.
- `proxy` (str, optional): Прокси-сервер для использования. По умолчанию `None`.
- `**kwargs`: Дополнительные аргументы, такие как `temperature`.

**Возвращает**:
- `AsyncResult`: Асинхронный генератор, выдающий ответы от API AiAsk.

**Вызывает исключения**:
- `RuntimeError`: Если достигнут лимит запросов.

**Как работает функция**:

1. **Подготовка заголовков**:
   - Создаются заголовки HTTP-запроса, включающие `accept`, `origin` и `referer`.
2. **Создание асинхронной сессии**:
   - Создается асинхронная сессия `aiohttp.ClientSession` с заданными заголовками.
3. **Подготовка данных для запроса**:
   - Формируются данные для отправки в формате JSON, включающие историю сообщений, модель, температуру и другие параметры.
4. **Отправка POST-запроса**:
   - Отправляется POST-запрос к API AiAsk с использованием асинхронной сессии и прокси (если указан).
5. **Обработка ответа**:
   - Читаются данные из ответа по частям (chunks) и передаются через генератор.
   - Проверяется наличие сообщения об ограничении скорости (`rate_limit`).
   - Если достигнут лимит запросов, выбрасывается исключение `RuntimeError`.
6. **Возврат результата**:
   - Возвращается асинхронный генератор, выдающий части ответа от API.

**ASII flowchart**:

```
    Заголовки запроса
    ↓
    Создание асинхронной сессии
    ↓
    Формирование данных запроса (JSON)
    ↓
    Отправка POST-запроса к API
    ↓
    Обработка ответа по частям (chunks)
    ├──> Проверка на лимит запросов
    │    └──> Выброс RuntimeError (если лимит достигнут)
    ↓
    Выдача части ответа через генератор
```

**Примеры**:

- Пример использования с минимальными параметрами:

```python
async for chunk in AiAsk.create_async_generator(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello"}]):
    print(chunk)
```

- Пример использования с указанием прокси и температуры:

```python
async for chunk in AiAsk.create_async_generator(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello"}], proxy="http://proxy.example.com", temperature=0.7):
    print(chunk)
```

- Пример обработки исключения при достижении лимита запросов:

```python
try:
    async for chunk in AiAsk.create_async_generator(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello"}]):
        print(chunk)
except RuntimeError as ex:
    print(f"Ошибка: {ex}")