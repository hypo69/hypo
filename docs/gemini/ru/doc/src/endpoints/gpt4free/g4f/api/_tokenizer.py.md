# Модуль для токенизации текста

## Обзор

Модуль предоставляет функциональность для токенизации текста с использованием библиотеки `tiktoken`. Он позволяет разбивать текст на токены и получать количество токенов, что полезно для оценки размера текста при работе с языковыми моделями.

## Подробней

Данный модуль предназначен для работы с токенами в текстовых данных. Он использует библиотеку `tiktoken` для кодирования текста и определения количества токенов. Это может быть полезно при работе с большими объемами текста, когда необходимо оценить его размер и сложность для обработки языковыми моделями.

## Функции

### `tokenize`

```python
# def tokenize(text: str, model: str = 'gpt-3.5-turbo') -> Union[int, str]:
#     encoding   = tiktoken.encoding_for_model(model)
#     encoded    = encoding.encode(text)
#     num_tokens = len(encoded)
#
#     return num_tokens, encoded
```

**Назначение**: Токенизирует входной текст с использованием указанной модели и возвращает количество токенов и закодированный текст.

**Параметры**:

- `text` (str): Текст для токенизации.
- `model` (str): Модель, используемая для токенизации (по умолчанию 'gpt-3.5-turbo').

**Возвращает**:

- `Union[int, str]`: Кортеж, содержащий количество токенов и закодированный текст.

**Как работает функция**:

1. **Получение кодировщика для модели**: Функция `tiktoken.encoding_for_model(model)` получает кодировщик, соответствующий указанной модели.
2. **Кодирование текста**: Метод `encoding.encode(text)` кодирует входной текст с использованием полученного кодировщика.
3. **Определение количества токенов**: Функция `len(encoded)` определяет количество токенов в закодированном тексте.
4. **Возврат результатов**: Функция возвращает кортеж, содержащий количество токенов и закодированный текст.

**Примеры**:
Примеры не предоставлены.
```