# Объяснение кода crawlee_python.py

Этот Python-код реализует веб-скрабер, использующий библиотеку `crawlee`.  Он позволяет собрать данные с веб-страниц, пройдя по ссылкам и извлекая информацию с помощью Playwright.

**Ключевые классы и функции:**

* **`CrawleePython`:** Класс, отвечающий за инициализацию, запуск и обработку данных извлечения.
    * **`__init__`:** Инициализирует экземпляр класса с параметрами: `max_requests` (максимальное количество запросов), `headless` (режим работы без графического интерфейса) и `browser_type` (тип браузера).
    * **`setup_crawler`:** Настраивает экземпляр `PlaywrightCrawler`.  Ключевым элементом является декорирование `@self.crawler.router.default_handler`  функции `request_handler`.  Это позволяет перехватывать и обрабатывать каждый запрос во время работы скрабера.
    * **`request_handler`:** Обработчик запросов. Логирует текущий URL.  Выполняет `enqueue_links` для добавления найденных ссылок в очередь.  Извлекает `title` и `content` с текущей страницы. Ключевой особенностью является ограничение `content` первыми 100 символами. Это важно для управления размером данных и предотвращения переполнения.  Данные помещаются в `default_dataset`.
    * **`run_crawler`:** Запускает сканирование с переданным списком URL.
    * **`export_data`:** Экспортирует собранные данные в файл JSON по указанному пути.
    * **`get_data`:** Возвращает собранные данные в формате словаря.
    * **`run`:**  Основной метод. Выполняет настройку `setup_crawler`, запуск `run_crawler`, экспорт `export_data`, получение данных `get_data` и вывод лога.
* **`main`:**  Внутри функции `if __name__ == '__main__':`  создает экземпляр класса `CrawleePython` и вызывает `run` для обработки списка `urls` (в данном случае, `https://ksp.co.il`).

**Используемые библиотеки:**

* **`crawlee`:** Библиотека для создания и управления веб-скраберами.
* **`PlaywrightCrawler`**  и `PlaywrightCrawlingContext`: компоненты для управления браузером Playwright в рамках Crawlee.
* **`asyncio`:** Библиотека для асинхронного программирования.
* **`pathlib`:** Для работы с файловыми путями.
* **`gs`:** Скорее всего, это пользовательская библиотека, определяющая константы или пути к файлам.
* **`logger`:** Библиотека для ведения логов.


**Как работает код:**

1.  Инициализируется `CrawleePython` с настройками.
2.  `setup_crawler` создает и настраивает `PlaywrightCrawler`.
3.  `run_crawler` запускает сканирование по заданным URL.
4.  `request_handler` обрабатывает каждый URL, извлекая title и часть содержимого.
5.  `export_data` сохраняет собранные данные в файле `results.json` в временной папке.
6.  `get_data` получает сохраненные данные.
7.  `run` объединяет эти шаги и завершает процесс.

**Важные моменты:**

* **`max_requests`:** Ограничение на количество запросов, предотвращающее перегрузку сервера.
* **`headless=False`:** Для запуска браузера с графическим интерфейсом.
* **Ограничение `content`:** Важно для предотвращения ошибок и сохранения размера данных.
* **Обработка ошибок:**  В коде отсутствует явная обработка потенциальных ошибок (например, проблемы с подключением, некорректные URL).  В реальных приложениях рекомендуется добавить механизмы обработки ошибок.
* **`gs.path.tmp`:**  Это предполагает, что `gs` предоставляет доступ к временной папке.
* **`logger`:** Используется для вывода информации о процессе.

Код хорошо структурирован и читаем.  Он демонстрирует хороший подход к обработке данных с помощью асинхронного программирования и библиотек, ориентированных на web scraping.