# Модуль `CrawleePython`

## Обзор

Модуль `CrawleePython` предоставляет класс `CrawleePython`, который использует библиотеку `crawlee` для выполнения веб-скрейпинга с использованием Playwright. Класс позволяет настроить краулер, извлекать данные со страниц и экспортировать их в JSON-файл.

## Оглавление

1.  [Класс `CrawleePython`](#класс-crawleepython)
    -   [Метод `__init__`](#__init__)
    -   [Метод `setup_crawler`](#setup_crawler)
    -   [Метод `run_crawler`](#run_crawler)
    -   [Метод `export_data`](#export_data)
    -   [Метод `get_data`](#get_data)
    -   [Метод `run`](#run)

## Класс `CrawleePython`

### **Описание**:

Класс `CrawleePython` реализует веб-скрейпер, использующий `PlaywrightCrawler` из библиотеки `crawlee`. Он позволяет настроить краулер, извлекать данные с веб-страниц и экспортировать их в JSON-файл.

### `__init__`

**Описание**:

Инициализирует экземпляр `CrawleePython` с заданными параметрами.

**Параметры**:

- `max_requests` (int): Максимальное количество запросов, которые будут выполнены во время обхода.
- `headless` (bool): Определяет, будет ли браузер запущен в режиме без GUI (headless mode). По умолчанию `True`.
- `browser_type` (str): Тип браузера для использования (например, 'chromium', 'firefox'). По умолчанию 'chromium'.

### `setup_crawler`

**Описание**:

Настраивает краулер, определяя обработчик запросов по умолчанию. Этот обработчик обрабатывает каждый запрос, извлекает данные со страницы и добавляет ссылки для дальнейшего обхода.

### `run_crawler`

**Описание**:

Запускает процесс обхода с заданным списком начальных URL-адресов.

**Параметры**:

- `start_urls` (list): Список начальных URL-адресов для обхода.

### `export_data`

**Описание**:

Экспортирует собранные данные в указанный JSON-файл.

**Параметры**:

- `output_file` (str): Путь к файлу, в который будут экспортированы данные.

### `get_data`

**Описание**:

Возвращает извлеченные данные в виде словаря.

**Возвращает**:

- `dict`: Словарь с извлеченными данными.

### `run`

**Описание**:

Оркестрирует весь процесс: настраивает краулер, запускает его, экспортирует данные и выводит извлеченные данные на экран.

**Параметры**:

- `start_urls` (list): Список начальных URL-адресов для обхода.
- `output_file` (str): Путь к файлу, в который будут экспортированы данные.

## Пример использования

Пример использования класса `CrawleePython` для сбора данных с сайта Hacker News:

```python
import asyncio
from crawlee_python import CrawleePython

async def main():
    crawler = CrawleePython(max_requests=10, headless=True)
    await crawler.run(
        start_urls=["https://news.ycombinator.com/"], output_file="hacker_news.json"
    )


if __name__ == "__main__":
    asyncio.run(main())

```