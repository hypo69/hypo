# Модуль `provider.py`

## Обзор

Модуль предоставляет класс `LocalProvider`, который используется для создания завершений текста с использованием локальных моделей GPT4All. Он определяет, где находятся файлы моделей, загружает их (если необходимо) и генерирует ответы на основе предоставленных сообщений.

## Подробней

Этот модуль является частью системы gpt4free, которая позволяет использовать различные провайдеры для генерации текста. `LocalProvider` использует локально установленные модели GPT4All. Он ищет файлы моделей в нескольких местах, включая локальный каталог, каталог проекта и рабочие каталоги. Если модель не найдена, он предлагает загрузить ее.

## Функции

### `find_model_dir`

```python
def find_model_dir(model_file: str) -> str:
    """
    Определяет каталог, в котором находится файл модели.

    Args:
        model_file (str): Имя файла модели.

    Returns:
        str: Путь к каталогу, содержащему файл модели.
    
    Как работает функция:
    1.  Функция `find_model_dir` принимает имя файла модели (`model_file`) в качестве аргумента и пытается найти каталог, содержащий этот файл.
    2.  Определяются три потенциальных местоположения для файла модели:
        - `local_dir`: Каталог, в котором находится текущий файл (`provider.py`).
        - `project_dir`: Родительский каталог `local_dir`.
        - `working_dir`: Текущий рабочий каталог.
    3.  Сначала проверяется, находится ли файл модели в подкаталоге `models` в `project_dir`. Если файл существует, возвращается путь к этому подкаталогу.
    4.  Если файл не найден в `project_dir`, проверяется наличие файла в подкаталоге `models` в `local_dir`. Если файл существует, возвращается путь к этому подкаталогу.
    5.  Если файл не найден ни в одном из вышеуказанных мест, выполняется обход (`os.walk`) текущего рабочего каталога (`./`) для поиска файла модели. Если файл найден в каком-либо подкаталоге, возвращается путь к этому подкаталогу.
    6.  Если файл не найден ни в одном из указанных мест, возвращается путь к подкаталогу `models` в `project_dir` в качестве каталога по умолчанию.
    """
```

### Класс `LocalProvider`

```python
class LocalProvider:
    """
    Провайдер для создания завершений текста с использованием локальных моделей GPT4All.
    """
```

#### `create_completion`

```python
    @staticmethod
    def create_completion(model: str, messages: Messages, stream: bool = False, **kwargs):
        """
        Создает завершение текста с использованием локальной модели GPT4All.

        Args:
            model (str): Имя модели для использования.
            messages (Messages): Список сообщений для передачи модели.
            stream (bool, optional): Флаг, указывающий, следует ли возвращать результат в виде потока. По умолчанию `False`.
            **kwargs: Дополнительные аргументы, которые могут быть переданы модели.

        Yields:
            str: Сгенерированный текст.

        Raises:
            ValueError: Если модель не найдена или не реализована.

        Как работает функция:
        1.  `create_completion` принимает имя модели (`model`), список сообщений (`messages`), флаг потоковой передачи (`stream`) и дополнительные аргументы (`kwargs`).
        2.  Проверяется, инициализирован ли глобальный `MODEL_LIST`. Если нет, он инициализируется с использованием `get_models()`.
        3.  Проверяется, существует ли запрошенная модель в `MODEL_LIST`. Если нет, вызывается `ValueError`.
        4.  Извлекаются путь к файлу модели (`model_file`) и каталог модели (`model_dir`) из `MODEL_LIST`.
        5.  Проверяется, существует ли файл модели в указанном каталоге. Если нет, пользователю предлагается загрузить модель. Если пользователь отказывается, вызывается `ValueError`.
        6.  Создается экземпляр `GPT4All` с указанным файлом модели, отключается подробный вывод и загрузка.
        7.  Извлекается системное сообщение из `messages` и форматируется.
        8.  Форматируется разговор из `messages` в строку, ожидаемую моделью.
        9.  Определяется функция `should_not_stop`, которая определяет, следует ли остановить генерацию текста на основе токенов.
        10. Используется `model.chat_session` для управления контекстом разговора.
        11. Если `stream` имеет значение `True`, генерируется текст потоком с использованием `model.generate`, и каждый токен возвращается с помощью `yield`. В противном случае генерируется полный текст и возвращается с помощью `yield`.
        """
```

## Переменные

### `MODEL_LIST`

```python
MODEL_LIST: dict[str, dict] = None
```

-   **Описание**: Глобальная переменная, содержащая словарь доступных локальных моделей. Ключи - это имена моделей, а значения - словари с информацией о моделях, такой как путь к файлу модели. Изначально установлена в `None` и заполняется при первом вызове `create_completion`.