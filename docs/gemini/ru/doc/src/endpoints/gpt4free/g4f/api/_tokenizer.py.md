# Модуль для токенизации текста (Временно не используется)

## Обзор

Модуль предназначен для токенизации текста с использованием библиотеки `tiktoken`. Он позволяет подсчитывать количество токенов в тексте, что полезно для оценки стоимости использования языковых моделей, таких как GPT-3.5 Turbo.

## Подробнее

Этот модуль предоставляет функцию `tokenize`, которая принимает текст и название модели в качестве входных данных. Функция использует `tiktoken` для кодирования текста и возвращает количество токенов и закодированный текст.

## Функции

### `tokenize`

```python
def tokenize(text: str, model: str = 'gpt-3.5-turbo') -> int | str:
    """ Функция токенизирует входной текст, используя указанную модель, и возвращает количество токенов и закодированный текст.

    Args:
        text (str): Текст для токенизации.
        model (str, optional): Название модели для токенизации. По умолчанию 'gpt-3.5-turbo'.

    Returns:
        int | str: Количество токенов в тексте и закодированный текст.

    Raises:
        Exception: Если возникает ошибка при токенизации текста.

    Example:
        >>> num_tokens, encoded = tokenize("Пример текста для токенизации", model='gpt-3.5-turbo')
        >>> print(f'Количество токенов: {num_tokens}')
        Количество токенов: 5
        >>> print(f'Закодированный текст: {encoded}')
        Закодированный текст: [99205, 11106, 15437, 11614, 99305]
    """
    ...
```

**Как работает функция**:

1. Функция `tokenize` принимает текст и модель в качестве параметров.
2. Она использует `tiktoken.encoding_for_model(model)` для получения кодировщика на основе указанной модели.
3. Затем она кодирует текст с помощью `encoding.encode(text)`, что преобразует текст в последовательность чисел (токенов).
4. Количество токенов определяется как длина закодированной последовательности.
5. Функция возвращает количество токенов и закодированный текст.

**Примеры**:

```python
from src.endpoints.gpt4free.g4f.api._tokenizer import tokenize

# Пример 1: Токенизация текста с использованием модели gpt-3.5-turbo
text = "Пример текста для токенизации."
num_tokens, encoded = tokenize(text, model='gpt-3.5-turbo')
print(f"Количество токенов: {num_tokens}")
print(f"Закодированный текст: {encoded}")

# Пример 2: Токенизация другого текста
text = "Это другой пример текста."
num_tokens, encoded = tokenize(text, model='gpt-3.5-turbo')
print(f"Количество токенов: {num_tokens}")
print(f"Закодированный текст: {encoded}")