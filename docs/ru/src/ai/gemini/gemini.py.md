# Модуль для взаимодействия с Google Generative AI

## Обзор

Модуль `gemini` предоставляет интеграцию с моделями Google Generative AI, такими как Gemini. Он содержит класс `GoogleGenerativeAI`, который позволяет взаимодействовать с этими моделями для выполнения различных задач, таких как генерация текста, описание изображений и ведение диалогов.

## Подробнее

Этот модуль позволяет удобно использовать возможности Google Gemini для работы с текстовыми и визуальными данными. Он включает в себя функции для настройки модели, отправки запросов и обработки ответов, а также для управления историей диалогов.

## Классы

### `Config`

**Описание**:
Этот класс предназначен для хранения конфигурационных данных. 
<Так как в предоставленном коде класс `Config` не реализован, то  здесь должно быть описание его полей и назначения, если бы он был.>

### `GoogleGenerativeAI`

**Описание**:
Класс `GoogleGenerativeAI` предоставляет интерфейс для взаимодействия с моделями Google Generative AI. Он позволяет отправлять текстовые запросы, описывать изображения и управлять историей диалогов.

**Как работает класс**:

1.  **Инициализация**: При инициализации класса `GoogleGenerativeAI` происходит настройка модели Gemini с использованием предоставленного API-ключа, имени модели и системных инструкций. Также создается объект чата для ведения диалогов.
2.  **Взаимодействие с моделью**: Класс предоставляет методы для отправки текстовых запросов (`ask`, `ask_async`), описания изображений (`describe_image`) и ведения диалогов (`chat`).
3.  **Управление историей диалогов**: Класс позволяет сохранять и загружать историю диалогов, а также очищать ее.

**Методы**:

*   `__post_init__`: Инициализация модели GoogleGenerativeAI с дополнительными настройками.
*   `normalize_answer`: Очистка вывода от различных типов форматирования (например, ````md`, ````python`, ````json`, ````html`).
*   `_start_chat`: Запуск чата с начальной настройкой.
*   `clear_history`: Очищает историю чата в памяти и удаляет файл истории, если он существует.
*   `_save_chat_history`: Сохраняет всю историю чата в JSON файл.
*   `_load_chat_history`: Загружает историю чата из JSON файла.
*   `chat`: Обрабатывает чат-запрос с различными режимами управления историей чата.
*   `ask`: Отправляет текстовый запрос модели и возвращает ответ.
*   `ask_async`: Асинхронно отправляет текстовый запрос модели и возвращает ответ.
*   `describe_image`: Отправляет изображение в Gemini Pro Vision и возвращает его текстовое описание.
*   `upload_file`: Загружает файл в Gemini.

**Параметры**:

*   `api_key` (str): API-ключ для доступа к Google Generative AI.
*   `model_name` (str): Имя используемой модели Gemini. По умолчанию `"gemini-2.0-flash-exp"`.
*   `generation_config` (Dict): Конфигурация генерации. По умолчанию `{"response_mime_type": "text/plain"}`.
*   `system_instruction` (Optional[str]): Системные инструкции для модели.
*   `chat_history` (List[Dict]): История чата.

## Функции

### `__post_init__`

```python
def __post_init__(self):
    """Инициализация модели GoogleGenerativeAI с дополнительными настройками."""
    ...
```

**Как работает функция**:

Функция `__post_init__` выполняет инициализацию модели `GoogleGenerativeAI` после создания экземпляра класса.

Внутри функции происходят следующие действия и преобразования:

1.  **Загрузка конфигурации**: Загружает конфигурацию из файла `gemini.json` с использованием функции `j_loads_ns`.
2.  **Определение путей к файлам**: Определяет пути к файлам для логирования диалогов и хранения истории чата.
3.  **Инициализация модели Gemini**: Инициализирует модель Gemini с использованием API-ключа, имени модели и системных инструкций.
4.  **Запуск чата**: Запускает чат с использованием метода `_start_chat`.

### `normalize_answer`

```python
def normalize_answer(self, text:str) -> str:
    """Очистка вывода от 
    ```md, ```python, ```json, ```html, ит.п.
    """
    ...
```

**Как работает функция**:

Функция `normalize_answer` очищает текстовый вывод модели Gemini от различных типов форматирования, таких как Markdown, Python, JSON и HTML.

Внутри функции происходят следующие действия и преобразования:

1.  **Вызов функции `normalize_answer`**: Вызывает функцию `normalize_answer` из модуля `src.utils.string.ai_string_normalizer` для выполнения очистки текста.

**Параметры**:

*   `text` (str): Текст, который необходимо очистить.

**Возвращает**:

*   `str`: Очищенный текст.

### `_start_chat`

```python
def _start_chat(self):
    """Запуск чата с начальной настройкой."""
    ...
```

**Как работает функция**:

Функция `_start_chat` запускает чат с моделью Gemini и выполняет начальную настройку.

Внутри функции происходят следующие действия и преобразования:

1.  **Проверка наличия системных инструкций**: Проверяет, заданы ли системные инструкции для модели.
2.  **Запуск чата с системными инструкциями**: Если системные инструкции заданы, запускает чат с использованием этих инструкций.
3.  **Запуск чата без системных инструкций**: Если системные инструкции не заданы, запускает чат без них.

**Возвращает**:

*   `Any`: Объект чата.

### `clear_history`

```python
def clear_history(self):
    """
    Очищает историю чата в памяти и удаляет файл истории, если он существует.
    """
    ...
```

**Как работает функция**:

Функция `clear_history` очищает историю чата, удаляя ее из памяти и удаляя соответствующий файл истории, если он существует.

Внутри функции происходят следующие действия и преобразования:

1.  **Очистка истории в памяти**: Очищает список `chat_history`, который хранит историю чата в памяти.
2.  **Удаление файла истории**: Проверяет, существует ли файл истории (`history_json_file`). Если файл существует, он удаляется.
3.  **Логирование**: В случае успешного удаления файла истории, записывает информационное сообщение в лог.
4.  **Обработка исключений**: В случае возникновения ошибки при очистке истории чата, записывает сообщение об ошибке в лог.

### `_save_chat_history`

```python
async def _save_chat_history(self, chat_data_folder: Optional[str | Path]):
    """Сохраняет всю историю чата в JSON файл"""
    ...
```

**Как работает функция**:

Функция `_save_chat_history` сохраняет историю чата в JSON-файл.

Внутри функции происходят следующие действия и преобразования:

1.  **Определение пути к файлу**: Если указана папка для хранения истории чата (`chat_data_folder`), формирует путь к файлу `history.json` в этой папке.
2.  **Сохранение истории чата**: Если история чата (`chat_history`) не пуста, сохраняет ее в JSON-файл с использованием функции `j_dumps`.

**Параметры**:

*   `chat_data_folder` (Optional[str | Path]): Папка для хранения истории чата.

### `_load_chat_history`

```python
async def _load_chat_history(self, chat_data_folder: Optional[str | Path]):
    """Загружает историю чата из JSON файла"""
    ...
```

**Как работает функция**:

Функция `_load_chat_history` загружает историю чата из JSON-файла.

Внутри функции происходят следующие действия и преобразования:

1.  **Определение пути к файлу**: Если указана папка для хранения истории чата (`chat_data_folder`), формирует путь к файлу `history.json` в этой папке.
2.  **Загрузка истории чата**: Если файл истории существует, загружает историю чата из файла с использованием функции `j_loads`.
3.  **Восстановление чата**: Восстанавливает состояние чата, добавляя записи из загруженной истории в текущий чат.
4.  **Логирование**: В случае успешной загрузки истории чата, записывает информационное сообщение в лог.
5.  **Обработка исключений**: В случае возникновения ошибки при загрузке истории чата, записывает сообщение об ошибке в лог.

**Параметры**:

*   `chat_data_folder` (Optional[str | Path]): Папка для хранения истории чата.

### `chat`

```python
async def chat(self, q: str, chat_data_folder: Optional[str | Path], flag: str = "save_chat") -> Optional[str]:
    """
    Обрабатывает чат-запрос с различными режимами управления историей чата.

    Args:
        q (str): Вопрос пользователя.
        chat_data_folder (Optional[str | Path]): Папка для хранения истории чата.
        flag (str): Режим управления историей. Возможные значения: 
                    "save_chat", "read_and_clear", "clear", "start_new".

    Returns:
        Optional[str]: Ответ модели.
    """
    ...
```

**Как работает функция**:

Функция `chat` обрабатывает чат-запрос с различными режимами управления историей чата.

Внутри функции происходят следующие действия и преобразования:

1.  **Обработка флагов**: В зависимости от значения флага `flag`, выполняет различные действия с историей чата:
    *   `"save_chat"`: Загружает историю чата из файла.
    *   `"read_and_clear"`: Загружает историю чата из файла и очищает ее.
    *   `"read_and_start_new"`: Загружает историю чата из файла, сохраняет ее и начинает новую историю.
    *   `"clear"`: Очищает историю чата.
    *   `"start_new"`: Сохраняет текущую историю чата в архивный файл и начинает новую историю.
2.  **Отправка запроса модели**: Отправляет запрос `q` модели Gemini с использованием метода `_chat.send_message_async`.
3.  **Обработка ответа**: Если получен ответ от модели, добавляет запрос и ответ в историю чата и сохраняет историю в файл.
4.  **Логирование**: В случае получения пустого ответа от модели, записывает сообщение об ошибке в лог.
5.  **Обработка исключений**: В случае возникновения ошибки при обработке чат-запроса, записывает сообщение об ошибке в лог.

**Параметры**:

*   `q` (str): Вопрос пользователя.
*   `chat_data_folder` (Optional[str | Path]): Папка для хранения истории чата.
*   `flag` (str): Режим управления историей чата. Возможные значения: `"save_chat"`, `"read_and_clear"`, `"clear"`, `"start_new"`.

**Возвращает**:

*   `Optional[str]`: Ответ модели Gemini.

### `ask`

```python
def ask(self, q: str, attempts: int = 15, save_history:bool = False, clean_response:bool = True) -> Optional[str]:
    """
    Метод отправляет текстовый запрос модели и возвращает ответ.
    """
    ...
```

**Как работает функция**:

Функция `ask` отправляет текстовый запрос модели Gemini и возвращает ответ.

Внутри функции происходят следующие действия и преобразования:

1.  **Повторные попытки**: Выполняет несколько попыток отправки запроса модели в случае возникновения ошибок.
2.  **Отправка запроса модели**: Отправляет запрос `q` модели Gemini с использованием метода `model.generate_content`.
3.  **Обработка ответа**: Если получен ответ от модели, проверяет, нужно ли сохранять историю диалога. Если нужно, сохраняет запрос и ответ в файл.
4.  **Очистка ответа**: Если `clean_response` равен `True`, очищает ответ от различных типов форматирования с использованием метода `normalize_answer`.
5.  **Логирование**: В случае отсутствия ответа от модели, записывает отладочное сообщение в лог.
6.  **Обработка исключений**: В случае возникновения различных типов ошибок (сетевые ошибки, ошибки аутентификации, ошибки API и т.д.), записывает сообщение об ошибке в лог.

**Параметры**:

*   `q` (str): Текстовый запрос.
*   `attempts` (int): Количество попыток отправки запроса. По умолчанию `15`.
*   `save_history` (bool): Флаг, указывающий, нужно ли сохранять историю диалога. По умолчанию `False`.
*   `clean_response` (bool): Флаг, указывающий, нужно ли очищать ответ от форматирования. По умолчанию `True`.

**Возвращает**:

*   `Optional[str]`: Ответ модели Gemini.

### `ask_async`

```python
async def ask_async(self, q: str, attempts: int = 15, save_history: bool = False, clean_response:bool = True) -> Optional[str]:
    """
    Метод асинхронно отправляет текстовый запрос модели и возвращает ответ.
    """
    ...
```

**Как работает функция**:

Функция `ask_async` асинхронно отправляет текстовый запрос модели Gemini и возвращает ответ.

Внутри функции происходят следующие действия и преобразования:

1.  **Повторные попытки**: Выполняет несколько попыток отправки запроса модели в случае возникновения ошибок.
2.  **Отправка запроса модели**: Отправляет запрос `q` модели Gemini с использованием метода `model.generate_content` в асинхронном режиме.
3.  **Обработка ответа**: Если получен ответ от модели, проверяет, нужно ли сохранять историю диалога. Если нужно, сохраняет запрос и ответ в файл.
4.  **Очистка ответа**: Если `clean_response` равен `True`, очищает ответ от различных типов форматирования с использованием метода `normalize_answer`.
5.  **Логирование**: В случае отсутствия ответа от модели, записывает отладочное сообщение в лог.
6.  **Обработка исключений**: В случае возникновения различных типов ошибок (сетевые ошибки, ошибки аутентификации, ошибки API и т.д.), записывает сообщение об ошибке в лог.

**Параметры**:

*   `q` (str): Текстовый запрос.
*   `attempts` (int): Количество попыток отправки запроса. По умолчанию `15`.
*   `save_history` (bool): Флаг, указывающий, нужно ли сохранять историю диалога. По умолчанию `False`.
*   `clean_response` (bool): Флаг, указывающий, нужно ли очищать ответ от форматирования. По умолчанию `True`.

**Возвращает**:

*   `Optional[str]`: Ответ модели Gemini.

### `describe_image`

```python
def describe_image(
    self, image: Path | bytes, mime_type: Optional[str] = 'image/jpeg', prompt: Optional[str] = ''
) -> Optional[str]:
    """
    Отправляет изображение в Gemini Pro Vision и возвращает его текстовое описание.

    Args:
        image: Путь к файлу изображения или байты изображения

    Returns:
        str: Текстовое описание изображения.
        None: Если произошла ошибка.
    """
    ...
```

**Как работает функция**:

Функция `describe_image` отправляет изображение в Gemini Pro Vision и возвращает его текстовое описание.

Внутри функции происходят следующие действия и преобразования:

1.  **Подготовка изображения**: Если `image` является путем к файлу, загружает изображение в виде байтов.
2.  **Формирование запроса**: Формирует запрос, содержащий изображение и текстовый промпт.
3.  **Отправка запроса модели**: Отправляет запрос модели Gemini с использованием метода `model.generate_content`.
4.  **Обработка ответа**: Если получен ответ от модели, возвращает текстовое описание изображения.
5.  **Логирование**: В случае возникновения ошибок, записывает сообщение об ошибке в лог.

**Параметры**:

*   `image` (Path | bytes): Путь к файлу изображения или байты изображения.
*   `mime_type` (Optional[str]): MIME-тип изображения. По умолчанию `'image/jpeg'`.
*   `prompt` (Optional[str]): Текстовый промпт для описания изображения. По умолчанию `''`.

**Возвращает**:

*   `Optional[str]`: Текстовое описание изображения.

### `upload_file`

```python
async def upload_file(
    self, file: str | Path | IOBase, file_name: Optional[str] = None
) -> bool:
    """
    https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/upload_file.md
    response (file_types.File)
    """
    ...
```

**Как работает функция**:

Функция `upload_file` загружает файл в Gemini.

Внутри функции происходят следующие действия и преобразования:

1.  **Загрузка файла**: Загружает файл в Gemini с использованием метода `genai.upload_file_async`.
2.  **Логирование**: В случае успешной загрузки файла, записывает отладочное сообщение в лог.
3.  **Обработка ошибок**: Если возникает ошибка при загрузке файла, пытается удалить файл и повторить загрузку. В случае возникновения общей ошибки модели, записывает сообщение об ошибке в лог.

**Параметры**:

*   `file` (str | Path | IOBase): Путь к файлу, который нужно загрузить.
*   `file_name` (Optional[str]): Имя файла.

**Возвращает**:

*   `bool`: `True`, если файл успешно загружен.

### `main`

```python
async def main():
    # Замените на свой ключ API

    system_instruction = "Ты - полезный ассистент. Отвечай на все вопросы кратко"
    ai = GoogleGenerativeAI(api_key=gs.credentials.gemini.api_key, system_instruction=system_instruction)

    # Пример вызова describe_image с промптом
    image_path = Path(r"test.jpg")  # Замените на путь к вашему изображению

    if not image_path.is_file():
        print(
            f"Файл {image_path} не существует. Поместите в корневую папку с программой файл с названием test.jpg"
        )
    else:
        prompt = """Проанализируй это изображение. Выдай ответ в формате JSON,
        где ключом будет имя объекта, а значением его описание.
         Если есть люди, опиши их действия."""

        description = await ai.describe_image(image_path, prompt=prompt)
        if description:
            print("Описание изображения (с JSON форматом):")
            print(description)
            try:
                parsed_description = j_loads(description)

            except Exception as ex:
                print("Не удалось распарсить JSON. Получен текст:")
                print(description)

        else:
            print("Не удалось получить описание изображения.")

        # Пример без JSON вывода
        prompt = "Проанализируй это изображение. Перечисли все объекты, которые ты можешь распознать."
        description = await ai.describe_image(image_path, prompt=prompt)
        if description:
            print("Описание изображения (без JSON формата):")
            print(description)

    file_path = Path('test.txt')
    with open(file_path, "w") as f:
        f.write("Hello, Gemini!")

    file_upload = await ai.upload_file(file_path, 'test_file.txt')
    print(file_upload)

    # Пример чата
    while True:
        user_message = input("You: ")
        if user_message.lower() == 'exit':
            break
        ai_message = await ai.chat(user_message)
        if ai_message:
            print(f"Gemini: {ai_message}")
        else:
            print("Gemini: Ошибка получения ответа")


if __name__ == "__main__":
    asyncio.run(main())
```

**Как работает функция**:

Функция `main` является точкой входа в программу и выполняет следующие действия:

1.  **Инициализация модели**: Инициализирует модель `GoogleGenerativeAI` с использованием API-ключа и системных инструкций.
2.  **Описание изображения**: Вызывает метод `describe_image` для получения описания изображения из файла `test.jpg`.
3.  **Загрузка файла**: Вызывает метод `upload_file` для загрузки файла `test.txt` в Gemini.
4.  **Чат**: Запускает цикл чата, в котором пользователь может вводить сообщения, а модель Gemini будет отвечать на них.

**Примеры**:

Примеры вызова с разными параметрами:

```python
# Пример вызова describe_image с промптом
image_path = Path(r"test.jpg")  # Замените на путь к вашему изображению

if not image_path.is_file():
    print(
        f"Файл {image_path} не существует. Поместите в корневую папку с программой файл с названием test.jpg"
    )
else:
    prompt = """Проанализируй это изображение. Выдай ответ в формате JSON,
    где ключом будет имя объекта, а значением его описание.
     Если есть люди, опиши их действия."""

    description = await ai.describe_image(image_path, prompt=prompt)
    if description:
        print("Описание изображения (с JSON форматом):")
        print(description)
        try:
            parsed_description = j_loads(description)

        except Exception as ex:
            print("Не удалось распарсить JSON. Получен текст:")
            print(description)

    else:
        print("Не удалось получить описание изображения.")

    # Пример без JSON вывода
    prompt = "Проанализируй это изображение. Перечисли все объекты, которые ты можешь распознать."
    description = await ai.describe_image(image_path, prompt=prompt)
    if description:
        print("Описание изображения (без JSON формата):")
        print(description)

file_path = Path('test.txt')
with open(file_path, "w") as f:
    f.write("Hello, Gemini!")

file_upload = await ai.upload_file(file_path, 'test_file.txt')
print(file_upload)

# Пример чата
while True:
    user_message = input("You: ")
    if user_message.lower() == 'exit':
        break
    ai_message = await ai.chat(user_message)
    if ai_message:
        print(f"Gemini: {ai_message}")
    else:
        print("Gemini: Ошибка получения ответа")
```