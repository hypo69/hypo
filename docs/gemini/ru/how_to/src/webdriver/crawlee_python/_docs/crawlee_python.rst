Как использовать класс CrawleePython для веб-скрапинга
============================================================================================

Описание
-------------------------
Класс `CrawleePython` предоставляет удобный способ выполнения задач веб-скрапинга с использованием библиотеки `crawlee` и Playwright.  Он настраивает, запускает и экспортирует данные, полученные с веб-страниц.  Класс обрабатывает запросы, извлекает данные с помощью селекторов Playwright и записывает результаты в JSON-файл.

Шаги выполнения
-------------------------
1. **Инициализация**: Создайте экземпляр класса `CrawleePython`, передав параметры для настройки процесса сбора данных, такие как максимальное количество запросов (`max_requests`), режим работы браузера (без графического интерфейса - `headless`) и тип браузера (`browser_type`).

2. **Настройка обработчика запросов (setup_crawler)**:  Этот метод настраивает процесс обработки каждого запроса, включающий извлечение данных с помощью Playwright. Он определяет, как обрабатывать страницы, собирать информацию и добавлять ссылки для последующего анализа.

3. **Запуск сбора данных (run_crawler)**: Этот метод запускает процесс сбора данных с указанным списком начальных URL-адресов.  Он обрабатывает каждую ссылку, извлекает данные и добавляет новые ссылки для продолжения процесса.

4. **Экспорт данных (export_data)**: Метод экспортирует собранные данные в указанный JSON-файл для дальнейшей работы.

5. **Получение данных (get_data)**: Метод возвращает извлечённые данные в виде словаря, доступного для дальнейшей обработки.

6. **Основной метод запуска (run)**: Метод объединяет все шаги: настройка, запуск, экспорт и вывод данных.

Пример использования
-------------------------
.. code-block:: python

    import asyncio
    from your_module import CrawleePython

    async def main():
        crawler = CrawleePython(max_requests=10, headless=True, browser_type='chromium')
        await crawler.setup_crawler()
        initial_urls = ['https://news.ycombinator.com/']
        await crawler.run_crawler(initial_urls)
        await crawler.export_data('data.json')
        data = crawler.get_data()
        print(data)

    if __name__ == "__main__":
        asyncio.run(main())