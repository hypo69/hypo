# Модуль `Berlin`

## Обзор

Модуль `Berlin` представляет собой асинхронный провайдер для работы с языковой моделью GPT-3.5 Turbo через API сервиса `ai.berlin4h.top`. Он обеспечивает функциональность для генерации текста на основе предоставленных сообщений, используя асинхронные запросы.

## Подробнее

Модуль предназначен для интеграции с другими частями проекта `hypotez`, где требуется взаимодействие с языковыми моделями для генерации текста. Он использует библиотеку `aiohttp` для выполнения асинхронных HTTP-запросов и предоставляет удобный интерфейс для отправки запросов и получения результатов.

## Классы

### `Berlin`

**Описание**:
Класс `Berlin` является асинхронным провайдером, который наследуется от `AsyncGeneratorProvider`. Он реализует методы для взаимодействия с API `ai.berlin4h.top` и получения сгенерированного текста на основе модели GPT-3.5 Turbo.

**Принцип работы**:
1.  **Инициализация**: При первом запросе создается токен аутентификации, который сохраняется для последующих запросов.
2.  **Формирование запроса**: На основе входных сообщений формируется запрос к API `ai.berlin4h.top` с использованием токена аутентификации.
3.  **Отправка запроса**: Асинхронно отправляется POST-запрос к API с данными запроса.
4.  **Обработка ответа**: Ответ от API обрабатывается по частям (chunks), извлекается содержимое и передается в виде асинхронного генератора.

**Методы**:

*   `create_async_generator`: Асинхронный генератор для получения сгенерированного текста.

## Функции

### `create_async_generator`

```python
@classmethod
async def create_async_generator(
    cls,
    model: str,
    messages: Messages,
    proxy: str = None,
    **kwargs
) -> AsyncResult:
    """
    Создает асинхронный генератор для получения сгенерированного текста от API `ai.berlin4h.top`.

    Args:
        cls: Ссылка на класс.
        model (str): Название модели, используемой для генерации (по умолчанию "gpt-3.5-turbo").
        messages (Messages): Список сообщений для отправки в модель.
        proxy (str, optional): Прокси-сервер для использования при отправке запроса. По умолчанию `None`.
        **kwargs: Дополнительные аргументы для передачи в API.

    Returns:
        AsyncResult: Асинхронный генератор, возвращающий сгенерированный текст.

    Raises:
        RuntimeError: Если возникает ошибка при обработке ответа от API.

    Example:
        >>> async for chunk in Berlin.create_async_generator(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello"}]):
        ...     print(chunk)
    """
    ...
```

**Назначение**:
Функция `create_async_generator` создает асинхронный генератор, который отправляет запросы к API `ai.berlin4h.top` и возвращает сгенерированный текст в виде последовательности чанков.

**Как работает функция**:

1.  **Проверка модели**: Если модель не указана, устанавливается значение по умолчанию "gpt-3.5-turbo".
2.  **Формирование заголовков**: Создаются заголовки для HTTP-запроса, включая User-Agent, Accept, Referer, Content-Type и другие.
3.  **Создание сессии**: Используется `aiohttp.ClientSession` для выполнения асинхронных запросов.
4.  **Получение токена**: Если токен отсутствует (`cls._token is None`), выполняется запрос к API для получения токена аутентификации.
    *   Формируются данные для запроса на логин, содержащие имя пользователя и пароль.
    *   Выполняется POST-запрос к эндпоинту `/api/login` для аутентификации.
    *   Токен извлекается из ответа и сохраняется в `cls._token`.
5.  **Формирование запроса на генерацию текста**:
    *   Формируется prompt (запрос) на основе предоставленных сообщений с использованием функции `format_prompt`.
    *   Создаются данные для запроса, включающие prompt, parentMessageId (случайный UUID), и опции модели (model, temperature, max_tokens и другие).
6.  **Отправка запроса и обработка ответа**:
    *   Выполняется POST-запрос к эндпоинту `/api/chat/completions` с данными запроса и заголовками, включающими токен.
    *   Ответ от API обрабатывается по частям (chunks).
    *   Из каждого чанка извлекается содержимое (`chunk["content"]`) и возвращается через `yield`.
    *   В случае ошибки при обработке ответа генерируется исключение `RuntimeError`.

**Примеры**:

```python
async for chunk in Berlin.create_async_generator(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hello"}]):
    print(chunk)