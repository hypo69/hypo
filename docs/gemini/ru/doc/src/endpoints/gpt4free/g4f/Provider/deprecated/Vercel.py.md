# Модуль Vercel

## Обзор

Модуль `Vercel` предоставляет реализацию для взаимодействия с моделями, размещенными на платформе Vercel. Он использует API Vercel для создания и получения ответов от этих моделей.
Модуль поддерживает как потоковую передачу ответов, так и работу с историей сообщений.

## Подробней

Этот модуль является частью системы провайдеров, используемой для абстрагирования взаимодействия с различными AI-моделями. Он предоставляет способ взаимодействия с моделями Vercel, включая настройку параметров запроса, отправку запросов и обработку ответов. Модуль предназначен для упрощения интеграции с Vercel AI SDK и обеспечения единообразного интерфейса для использования различных моделей.

## Классы

### `Vercel(AbstractProvider)`

**Описание**: Класс `Vercel` является реализацией абстрактного провайдера `AbstractProvider` для взаимодействия с платформой Vercel.

**Наследует**:
- `AbstractProvider`: Абстрактный базовый класс для всех провайдеров.

**Атрибуты**:
- `url` (str): URL для взаимодействия с Vercel AI SDK.
- `working` (bool): Флаг, указывающий, работает ли провайдер.
- `supports_message_history` (bool): Флаг, указывающий, поддерживает ли провайдер историю сообщений.
- `supports_gpt_35_turbo` (bool): Флаг, указывающий, поддерживает ли провайдер модель `gpt-3.5-turbo`.
- `supports_stream` (bool): Флаг, указывающий, поддерживает ли провайдер потоковую передачу ответов.

**Методы**:
- `create_completion(model: str, messages: Messages, stream: bool, proxy: str = None, **kwargs) -> CreateResult`:
  Создает завершение с использованием указанной модели, сообщений и параметров.
- `get_anti_bot_token() -> str`: Получает токен для защиты от ботов.

## Функции

### `create_completion`

```python
def create_completion(
    model: str,
    messages: Messages,
    stream: bool,
    proxy: str = None,
    **kwargs
) -> CreateResult:
    """
    Создает запрос на завершение текста к API Vercel.

    Args:
        model (str): Идентификатор модели для использования.
        messages (Messages): Список сообщений для отправки в модель.
        stream (bool): Флаг, указывающий, следует ли использовать потоковую передачу.
        proxy (str, optional): Прокси-сервер для использования. По умолчанию `None`.
        **kwargs: Дополнительные параметры для передачи в API.

    Returns:
        CreateResult: Генератор токенов или строка с результатом.

    Raises:
        MissingRequirementsError: Если не установлен пакет "PyExecJS".
        ValueError: Если указанная модель не поддерживается Vercel.

    """
```

**Назначение**:
Функция `create_completion` отправляет запрос к API Vercel для генерации текста на основе предоставленных входных данных (сообщения) и параметров модели. Она поддерживает потоковую передачу результатов, что позволяет получать ответы в режиме реального времени.

**Параметры**:
- `model` (str): Идентификатор модели, которую необходимо использовать для генерации текста. Например, "gpt-3.5-turbo".
- `messages` (Messages): Список сообщений, отправляемых в модель. Каждое сообщение содержит роль (например, "user" или "assistant") и контент.
- `stream` (bool): Флаг, определяющий, будет ли ответ возвращаться в виде потока токенов. Если `True`, функция возвращает генератор, который выдает токены по мере их поступления.
- `proxy` (str, optional): URL прокси-сервера, через который следует отправлять запросы. По умолчанию `None`.
- `**kwargs`: Дополнительные параметры, которые могут быть переданы в API Vercel для настройки генерации текста.

**Возвращает**:
- `CreateResult`: Если `stream` равен `True`, возвращает генератор, выдающий токены по мере их поступления. В противном случае возвращает строку с полным сгенерированным текстом.

**Вызывает исключения**:
- `MissingRequirementsError`: Если отсутствует необходимый пакет `PyExecJS`.
- `ValueError`: Если указанная модель не поддерживается Vercel.

**Как работает функция**:

1. **Проверка зависимостей**:
   - Проверяет, установлен ли пакет `PyExecJS`, необходимый для выполнения JavaScript-кода. Если пакет отсутствует, выбрасывается исключение `MissingRequirementsError`.

2. **Настройка модели**:
   - Если модель не указана, устанавливает значение по умолчанию "gpt-3.5-turbo".
   - Проверяет, поддерживается ли указанная модель Vercel. Если модель не поддерживается, выбрасывается исключение `ValueError`.

3. **Формирование заголовков запроса**:
   - Определяет заголовки HTTP-запроса, включая токен защиты от ботов, полученный с помощью функции `get_anti_bot_token()`.
   - Заголовки содержат информацию о браузере пользователя, типе контента и другие параметры, необходимые для успешного взаимодействия с API Vercel.

4. **Формирование тела запроса**:
   - Создает словарь `json_data`, содержащий параметры запроса, включая идентификатор модели, сообщения и дополнительные параметры, переданные через `kwargs`.

5. **Отправка запроса и обработка ответа**:
   - Отправляет POST-запрос к API Vercel (`https://chat.vercel.ai/api/chat`) с использованием библиотеки `requests`.
   - Если `stream` равен `True`, устанавливает параметр `stream=True` для получения ответа в виде потока.
   - В цикле пытается отправить запрос до `max_retries` раз (по умолчанию 20). В случае ошибки (например, HTTP-ошибка) пытается повторить запрос.
   - Если запрос успешен, итерируется по содержимому ответа (по токенам) и возвращает каждый токен с помощью `yield`.

**Внутренние функции**:
- Внутри данной функции нет внутренних функций. Она вызывает `get_anti_bot_token()` для получения токена.

**ASCII flowchart**:

```
[Проверка зависимостей]
    |
    V
[Настройка модели]
    |
    V
[Формирование заголовков запроса]
    |
    V
[Формирование тела запроса]
    |
    V
[Отправка запроса]
    |
    V
[Обработка ответа (потоковая передача)]
    |
    V
[Возврат токена]
```

**Примеры**:

```python
# Пример 1: Создание запроса с потоковой передачей
model = "gpt-3.5-turbo"
messages = [{"role": "user", "content": "Hello, how are you?"}]
stream = True
result = Vercel.create_completion(model=model, messages=messages, stream=stream)
for token in result:
    print(token, end="")

# Пример 2: Создание запроса без потоковой передачи
model = "gpt-3.5-turbo"
messages = [{"role": "user", "content": "Tell me a joke."}]
stream = False
result = Vercel.create_completion(model=model, messages=messages, stream=stream)
print(result)

# Пример 3: Использование прокси-сервера
model = "gpt-3.5-turbo"
messages = [{"role": "user", "content": "What is the capital of France?"}]
stream = True
proxy = "http://your-proxy-server:8080"
result = Vercel.create_completion(model=model, messages=messages, stream=stream, proxy=proxy)
for token in result:
    print(token, end="")
```

### `get_anti_bot_token`

```python
def get_anti_bot_token() -> str:
    """
    Получает токен для защиты от ботов с сайта sdk.vercel.ai.

    Args:
        None

    Returns:
        str: Закодированный токен для защиты от ботов.

    Raises:
        None
    """
```

**Назначение**:
Функция `get_anti_bot_token` извлекает и генерирует токен защиты от ботов, который используется для аутентификации запросов к API Vercel. Этот токен необходим для обхода защиты от автоматических запросов и обеспечения доступа к API.

**Параметры**:
- Нет параметров.

**Возвращает**:
- `str`: Закодированный токен защиты от ботов в формате Base64.

**Вызывает исключения**:
- Нет исключений.

**Как работает функция**:

1. **Формирование заголовков запроса**:
   - Определяет заголовки HTTP-запроса, включая информацию о браузере пользователя, типе контента и другие параметры, необходимые для успешного взаимодействия с API Vercel.

2. **Получение данных**:
   - Отправляет GET-запрос к API Vercel (`https://sdk.vercel.ai/openai.jpeg`) для получения данных, необходимых для генерации токена.

3. **Извлечение данных и формирование JavaScript-скрипта**:
   - Извлекает данные из ответа, декодирует их из формата Base64 и загружает в формате JSON.
   - Формирует JavaScript-скрипт, который использует полученные данные для генерации токена.

4. **Выполнение JavaScript-скрипта**:
   - Использует библиотеку `execjs` для выполнения JavaScript-скрипта и получения токена.

5. **Формирование и кодирование токена**:
   - Формирует JSON-структуру, содержащую токен и дополнительную информацию.
   - Кодирует JSON-структуру в формат Base64 и возвращает закодированный токен.

**Внутренние функции**:
- Внутри данной функции нет внутренних функций.

**ASCII flowchart**:

```
[Формирование заголовков запроса]
    |
    V
[Получение данных]
    |
    V
[Извлечение данных и формирование JavaScript-скрипта]
    |
    V
[Выполнение JavaScript-скрипта]
    |
    V
[Формирование и кодирование токена]
    |
    V
[Возврат токена]
```

**Примеры**:

```python
# Пример 1: Получение токена защиты от ботов
token = Vercel.get_anti_bot_token()
print(token)
```

### `ModelInfo`
```python
class ModelInfo(TypedDict):
    id: str
    default_params: dict[str, Any]
```

**Описание**:
`ModelInfo` - это класс `TypedDict`, используемый для определения структуры данных, содержащей информацию о моделях.

**Атрибуты**:
- `id` (str): Идентификатор модели.
- `default_params` (dict[str, Any]): Словарь с параметрами по умолчанию для модели.

### `model_info`
```python
model_info: dict[str, ModelInfo] = {
    # \'claude-instant-v1\': {
    #     \'id\': \'anthropic:claude-instant-v1\',\
    #     \'default_params\': {\
    #         \'temperature\': 1,\
    #         \'maximumLength\': 1024,\
    #         \'topP\': 1,\
    #         \'topK\': 1,\
    #         \'presencePenalty\': 1,\
    #         \'frequencyPenalty\': 1,\
    #         \'stopSequences\': [\'\\n\\nHuman:\'],\
    #     },\
    # },
...
}
```

**Описание**:
Словарь `model_info` содержит информацию о различных моделях, поддерживаемых провайдером Vercel. Каждая запись в словаре содержит идентификатор модели и параметры по умолчанию для этой модели.