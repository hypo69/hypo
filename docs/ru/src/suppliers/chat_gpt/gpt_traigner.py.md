# Модуль gpt_traigner

## Обзор

Модуль `gpt_traigner.py` предназначен для сбора и обработки данных разговоров из ChatGPT с целью дальнейшего обучения моделей искусственного интеллекта. Он включает в себя функциональность для извлечения текста из HTML-файлов, сохранения обработанных данных в различных форматах (CSV, JSONL, TXT) и использования этих данных для обучения модели.

## Подробней

Модуль `gpt_traigner` используется для автоматизации процесса сбора и подготовки данных для обучения моделей ИИ на основе разговоров, полученных из ChatGPT. Он автоматизирует извлечение данных из локально сохраненных HTML-файлов, содержащих историю разговоров, и преобразует их в структурированные форматы, пригодные для машинного обучения. Собранные данные могут быть использованы для улучшения качества ответов и адаптации моделей к специфическим потребностям.

## Классы

### `GPT_Traigner`

**Описание**: Класс `GPT_Traigner` предназначен для сбора, обработки и сохранения данных разговоров из ChatGPT. Он использует веб-драйвер для извлечения данных из HTML-файлов, содержащих историю разговоров, и сохраняет их в различных форматах для дальнейшего использования в обучении моделей ИИ.

**Принцип работы**:

1.  Инициализация класса создает экземпляр класса `GptGs`.
2.  Метод `dump_downloaded_conversations` выполняет основные этапы сбора и обработки данных:
    *   Определяет местоположение HTML-файлов с разговорами.
    *   Извлекает содержимое из каждого файла, используя веб-драйвер.
    *   Формирует структурированные данные, разделяя реплики пользователя и ассистента.
    *   Сохраняет полученные данные в форматах CSV, JSONL и TXT.

**Аттрибуты**:

*   `driver`: Экземпляр веб-драйвера (Chrome), используемый для навигации и извлечения данных с веб-страниц.
*   `gs`: Экземпляр класса `GptGs`, который, вероятно, предоставляет доступ к общим ресурсам и настройкам, связанным с Google Services.

**Методы**:

*   `__init__`: Инициализирует класс `GPT_Traigner`, создавая экземпляр класса `GptGs`.
*   `determine_sentiment`: Определяет тональность (sentiment) для пары сообщений в разговоре.
*   `save_conversations_to_jsonl`: Сохраняет список пар сообщений в формате JSONL.
*   `dump_downloaded_conversations`: Собирает разговоры со страниц ChatGPT, извлекая данные из HTML-файлов, и сохраняет их в форматах CSV, JSONL и TXT.

## Функции

### `determine_sentiment`

```python
def determine_sentiment(self, conversation_pair: dict[str, str], sentiment: str = 'positive') -> str:
    """ Determine sentiment label for a conversation pair """
    ...
```

**Назначение**: Определяет тональность (sentiment) для пары сообщений в разговоре.

**Параметры**:

*   `conversation_pair` (dict[str, str]): Словарь, содержащий пару сообщений для анализа тональности.
*   `sentiment` (str, optional): Тональность. По умолчанию 'positive'.

**Возвращает**:

*   `str`: "positive" или "negative" в зависимости от значения параметра `sentiment`.

**Как работает функция**:

Функция `determine_sentiment` определяет тональность (sentiment) для пары сообщений в разговоре. В текущей реализации, если параметр `sentiment` имеет значение (не пустая строка), функция возвращает "positive". В противном случае возвращается "negative".

```
Определение_тональности
│
├───> sentiment_есть?
│   │
│   └───> Да: "positive"
│   │
└───> Нет: "negative"
```

**Примеры**:

```python
traigner = GPT_Traigner()
print(traigner.determine_sentiment({"user": "hello", "assistant": "hi"}, sentiment=""))  # Вывод: negative
print(traigner.determine_sentiment({"user": "hello", "assistant": "hi"}, sentiment="not empty"))  # Вывод: positive
```

### `save_conversations_to_jsonl`

```python
def save_conversations_to_jsonl(self, data: list[dict], output_file: str):
    """ Save conversation pairs to a JSONL file """
    ...
```

**Назначение**: Сохраняет список пар сообщений в формате JSONL.

**Параметры**:

*   `data` (list[dict]): Список словарей, содержащих данные разговоров.
*   `output_file` (str): Путь к файлу, в который будут сохранены данные.

**Как работает функция**:

Функция `save_conversations_to_jsonl` принимает список словарей `data`, где каждый словарь представляет собой пару сообщений (например, вопрос пользователя и ответ ассистента), и сохраняет эти данные в файл в формате JSONL (JSON Lines). JSONL — это формат, где каждая строка файла является отдельным JSON-объектом.

Функция открывает файл с указанным именем (`output_file`) в режиме записи (`'w'`) и кодировкой UTF-8. Затем, для каждого элемента (словаря) в списке `data`, функция выполняет следующие действия:

1.  Очищает данные, используя функцию `clean_string(item)`. Это необходимо для удаления или замены специальных символов, которые могут вызвать проблемы при обработке текста.
2.  Преобразует очищенный словарь в JSON-строку с помощью `j_dumps(clean_string(item))`.
3.  Записывает JSON-строку в файл, добавляя символ новой строки (`\n`) в конце, чтобы каждый объект JSON был на отдельной строке.

```
Сохранение_разговоров_в_JSONL
│
├───> Открытие_файла (output_file, 'w', encoding='utf-8')
│
├───> Для каждого item в data:
│   │
│   ├───> Очистка_данных (clean_string(item))
│   │
│   ├───> Преобразование_в_JSON (j_dumps(clean_string(item)))
│   │
│   └───> Запись_в_файл (JSON + '\n')
│
└───> Закрытие_файла
```

**Примеры**:

```python
data = [{'user': 'Hello', 'assistant': 'Hi'}, {'user': 'How are you?', 'assistant': 'I am fine.'}]
output_file = 'conversations.jsonl'
traigner = GPT_Traigner()
traigner.save_conversations_to_jsonl(data, output_file)
```

### `dump_downloaded_conversations`

```python
def dump_downloaded_conversations(self):
    """ Collect conversations from the chatgpt page """
    ...
```

**Назначение**: Собирает разговоры со страниц ChatGPT, извлекая данные из HTML-файлов, и сохраняет их в форматах CSV, JSONL и TXT.

**Как работает функция**:

Функция `dump_downloaded_conversations` выполняет следующие шаги:

1.  **Определение местоположения HTML-файлов**:
    *   Определяет директорию, в которой хранятся HTML-файлы с сохраненными разговорами ChatGPT.

2.  **Перебор HTML-файлов**:
    *   Использует `glob("*.html")` для получения списка всех HTML-файлов в указанной директории.
    *   Инициализирует пустой список `all_data` для хранения обработанных данных из всех файлов.
    *   Устанавливает счетчик `counter` для отслеживания количества обработанных файлов.

3.  **Извлечение данных из каждого файла**:
    *   Для каждого `local_file_path` в списке `html_files` выполняются следующие действия:
        *   Формируется URI файла (`file_uri`) для открытия его в веб-драйвере.
        *   Веб-драйвер открывает файл, используя `self.driver.get_url(file_uri)`.
        *   Извлекаются элементы, содержащие реплики пользователя и ассистента, с использованием локаторов `locator.user` и `locator.assistant` через метод `self.driver.execute_locator`.
        *   Текстовое содержимое реплик извлекается из найденных элементов. Если `user_elements` или `assistant_elements` являются списком, то извлекается текст из каждого элемента списка. Если это один элемент, то извлекается текст из этого элемента. Если элементы не найдены, переменным присваивается значение `None`.

4.  **Обработка извлеченных данных**:
    *   Проверяется, что извлечены данные пользователя и ассистента. Если оба списка пусты, в лог записывается сообщение об ошибке, и происходит переход к следующему файлу.
    *   Используется функция `zip_longest` для итерации по репликам пользователя и ассистента. Если один из списков длиннее другого, `zip_longest` будет дополнять короткий список значением `None`.
    *   Для каждой пары реплик (user_text, assistant_text) создается словарь `data`, содержащий роли ('user', 'assistant'), содержимое реплик и тональность ('neutral', 'neutral').
    *   Этот словарь преобразуется в DataFrame с помощью `pd.DataFrame(data)`, и добавляется в список `all_data`.
    *   Выводится сообщение в консоль с указанием номера обработанного файла и его пути.

5.  **Сохранение обработанных данных**:
    *   После обработки всех файлов, если в списке `all_data` есть данные, они объединяются в один DataFrame с помощью `pd.concat(all_data, ignore_index=True)`.
    *   Полученный DataFrame сохраняется в трех форматах:
        *   CSV: `all_data_df.to_csv(csv_file_path, index=False, encoding='utf-8')`
        *   JSONL: `all_data_df.to_json(jsonl_file_path, orient='records', lines=True, force_ascii=False)`
        *   TXT: Из DataFrame извлекается столбец 'content', удаляются пропущенные значения (NaN), и все реплики объединяются в одну строку, которая сохраняется в файл `raw_conversations.txt`.

```
Сбор_и_сохранение_разговоров
│
├───> Определение_директории_с_HTML
│
├───> Получение_списка_HTML_файлов
│
├───> Для каждого HTML_файла:
│   │
│   ├───> Открытие_файла_в_браузере
│   │
│   ├───> Извлечение_реплик_пользователя_и_ассистента
│   │
│   ├───> Для каждой пары реплик:
│   │   │
│   │   ├───> Создание_словаря_данных
│   │   │
│   │   └───> Добавление_в_общий_список
│   │
│   └───> Сохранение_номера_и_пути_файла
│
└───> После_обработки_всех_файлов:
│   │
│   ├───> Объединение_данных_в_DataFrame
│   │
│   ├───> Сохранение_в_CSV
│   │
│   ├───> Сохранение_в_JSONL
│   │
│   └───> Сохранение_в_TXT
```

**Примеры**:

```python
traigner = GPT_Traigner()
traigner.dump_downloaded_conversations()