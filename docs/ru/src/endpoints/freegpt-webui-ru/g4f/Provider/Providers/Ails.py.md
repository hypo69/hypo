# Модуль `Ails.py`

## Обзор

Модуль предоставляет класс для взаимодействия с провайдером Ails, использующим модель `gpt-3.5-turbo`. Он содержит функции для форматирования запросов и обработки ответов от API `ai.ls`. Модуль поддерживает потоковую передачу данных и требует настройки заголовков для аутентификации и идентификации клиента.

## Подробнее

Модуль `Ails.py` предназначен для обеспечения взаимодействия с API `ai.ls` для получения ответов от модели `gpt-3.5-turbo`. Он включает в себя утилиты для создания хешей, форматирования временных меток и отправки запросов к API. Этот код играет важную роль в проекте, обеспечивая функциональность чат-бота через данного конкретного провайдера.

## Классы

### `Utils`

**Описание**: Класс `Utils` содержит статические методы для выполнения различных вспомогательных задач, таких как хеширование данных и форматирование временных меток.

**Принцип работы**:
Класс предоставляет два статических метода: `hash` и `format_timestamp`. Метод `hash` используется для создания SHA256 хеша на основе переданных данных, а метод `format_timestamp` форматирует временную метку.

**Методы**:
- `hash(json_data: Dict[str, str]) -> sha256`: Вычисляет SHA256 хеш для переданных данных JSON.
- `format_timestamp(timestamp: int) -> str`: Форматирует временную метку.

### `Utils.hash`

```python
def hash(json_data: Dict[str, str]) -> sha256:
    """
    Вычисляет SHA256 хеш для переданных данных JSON.

    Args:
        json_data (Dict[str, str]): Словарь с данными для хеширования.

    Returns:
        sha256: SHA256 хеш в шестнадцатеричном формате.

    """
```

**Как работает функция**:

1.  **Подготовка данных**: Извлекает значения `t` и `m` из `json_data`, а также использует секретный ключ и длину `json_data['m']` для формирования строки.
2.  **Формирование базовой строки**: Соединяет извлеченные значения и секретные данные в одну строку.
3.  **Хеширование**: Вычисляет SHA256 хеш от базовой строки.

```
json_data  -> Извлечение 't', 'm' -> Формирование base_string (с секретным ключом) -> SHA256 Hash
```

### `Utils.format_timestamp`

```python
def format_timestamp(timestamp: int) -> str:
    """
    Форматирует временную метку.

    Args:
        timestamp (int): Временная метка в миллисекундах.

    Returns:
        str: Форматированная временная метка в виде строки.
    """
```

**Как работает функция**:

1.  **Получение остатка**: Вычисляет остаток от деления `timestamp` на 10.
2.  **Корректировка остатка**: Если остаток четный, увеличивает его на 1.
3.  **Форматирование**: Вычитает исходный остаток из `timestamp` и добавляет скорректированный остаток, затем преобразует результат в строку.

```
timestamp -> Вычисление остатка (n) -> Корректировка остатка (r) -> timestamp - n + r -> Строка
```

## Функции

### `_create_completion`

```python
def _create_completion(model: str, messages: list, temperature: float = 0.6, stream: bool = False, **kwargs):
    """
    Создает запрос к API для получения ответа от модели.

    Args:
        model (str): Имя модели.
        messages (list): Список сообщений для отправки.
        temperature (float, optional): Температура для управления случайностью ответов. По умолчанию 0.6.
        stream (bool, optional): Включить потоковую передачу данных. По умолчанию False.
        **kwargs: Дополнительные аргументы.

    Returns:
        Generator[str, None, None]: Генератор токенов ответа, если stream=True.

    Raises:
        requests.exceptions.RequestException: В случае ошибки при отправке запроса.
    """
```

**Назначение**: Функция `_create_completion` отправляет запрос к API `caipacity.com` для получения ответа от языковой модели. Она формирует заголовки, параметры и данные запроса, а затем отправляет POST-запрос.

**Параметры**:

-   `model` (str): Имя используемой модели (например, 'gpt-3.5-turbo').
-   `messages` (list): Список сообщений для отправки в API.
-   `temperature` (float, optional): Параметр, определяющий "температуру" модели, влияющий на случайность генерируемого текста. По умолчанию равен 0.6.
-   `stream` (bool, optional): Указывает, следует ли использовать потоковый режим для получения ответа. По умолчанию `False`.
-   `**kwargs`: Дополнительные именованные аргументы, которые могут быть переданы в функцию.

**Возвращает**:

-   `Generator[str, None, None]`: Генератор, возвращающий токены ответа, если `stream=True`. Каждый токен представляет собой часть сгенерированного текста.

**Вызывает исключения**:

-   `requests.exceptions.RequestException`: В случае ошибки при отправке запроса.

**Внутренние функции**:

Внутри функции `_create_completion` нет внутренних функций.

**Как работает функция**:

1.  **Настройка заголовков**: Функция начинает с определения необходимых HTTP-заголовков, которые будут включены в запрос. Эти заголовки включают информацию о клиенте, типе контента, авторизации и user-agent.
2.  **Определение параметров**: Задаются параметры запроса, такие как `full`, который определяет, нужно ли возвращать полный ответ.
3.  **Форматирование временной метки**: Используется класс `Utils` для форматирования временной метки.
4.  **Создание подписи (sig)**: Создается подпись запроса, включающая дату, временную метку и хеш сообщения.
5.  **Формирование данных запроса (json_data)**: Создается JSON-объект, включающий модель, температуру, флаг потоковой передачи, сообщения и подпись. Этот объект сериализуется в строку.
6.  **Отправка POST-запроса**: Отправляется POST-запрос к API `caipacity.com` с использованием сформированных заголовков и данных.
7.  **Обработка потоковых ответов**: Если включен режим потоковой передачи (`stream=True`), функция итерируется по строкам ответа, извлекает контент из JSON и возвращает его в виде токенов с использованием `yield`.

```
Настройка заголовков, параметров -> Форматирование временной метки ->  Создание подписи -> Формирование JSON -> POST-запрос -> Обработка потока (извлечение токенов)
```

**Примеры**:

1.  Пример вызова функции с минимальным набором параметров:

```python
messages = [{"role": "user", "content": "Hello"}]
result = _create_completion(model="gpt-3.5-turbo", messages=messages)
for token in result:
    print(token, end="")
```

2.  Пример вызова функции с указанием температуры и включенным потоковым режимом:

```python
messages = [{"role": "user", "content": "Tell me a story"}]
result = _create_completion(model="gpt-3.5-turbo", messages=messages, temperature=0.8, stream=True)
for token in result:
    print(token, end="")
```

### `params`

```python
params = f'g4f.Providers.{os.path.basename(__file__)[:-3]} supports: ' + \
    '(%s)' % ', '.join([f"{name}: {get_type_hints(_create_completion)[name].__name__}" for name in _create_completion.__code__.co_varnames[:_create_completion.__code__.co_argcount]])
```

**Назначение**: Строка `params` формирует описание поддерживаемых параметров функции `_create_completion`.

**Как работает**:
1.  Извлекает имя файла текущего модуля, обрезает расширение `.py`.
2.  Получает имена и типы параметров функции `_create_completion` с помощью `get_type_hints`.
3.  Форматирует строку с именами параметров и их типами.
```
Извлечение имени файла -> Получение типов параметров -> Форматирование строки
```
**Примеры**:
```python
print(params) # g4f.Providers.Ails supports: (model: str, messages: list, temperature: float, stream: bool)